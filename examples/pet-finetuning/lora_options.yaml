architecture:
  name: "pet"
  training:
    num_epochs: 1  # very short period for demostration
    learning_rate: 1e-5  # small learning rate to stabilize training
    finetune:
      method: "lora"  # use LoRA fine-tuning strategy
      read_from: pet-mad-latest.ckpt  # path to the pretrained checkpoint to start from
      config:
        alpha: 0.1
        rank: 4

training_set:
  systems: 
    read_from: "data/ethanol_train.xyz"  # path to the finetuning dataset
    length_unit: angstrom
  targets:
    energy:
      key: "energy-corrected"  # name of the target value
      unit: "eV"

validation_set:
  systems: 
    read_from: "data/ethanol_val.xyz"  # path to the finetuning dataset
    length_unit: angstrom
  targets:
    energy:
      key: "energy-corrected"  # name of the target value
      unit: "eV"

test_set:
  systems: 
    read_from: "data/ethanol_test.xyz"  # path to the finetuning dataset
    length_unit: angstrom
  targets:
    energy:
      key: "energy-corrected"  # name of the target value
      unit: "eV"
