"""
Fine-tuning the PET-MAD universal potential
===========================================

:Authors: Davide Tisi `@DavideTisi <https://github.com/DavideTisi>`_,
          Zhiyi Wang `@0WellyWang0 <https://github.com/0WellyWang0>`_,
          Cesare Malosso `@cesaremalosso <https://github.com/cesaremalosso>`_

This example demonstrates how to finetune the PET-MAD model with metatrain,
`metatrain <https://github.com/metatensor/metatrain>`_.
PET-MAD is a "universal"
machine-learning forcefield trained on a dataset that aims to incorporate a very high
degree of structural diversity.

The point-edge transformer (PET) is an unconstrained architecture that achieves a high
degree of symmetry compliance through data augmentation during training (see the
`PET paper
<https://proceedings.neurips.cc/paper_files/paper/2023/file/fb4a7e3522363907b26a86cc5be627ac-Paper-Conference.pdf>`_
for more details). The unconstrained nature of the model simplifies its implementation
and structure, making it computationally efficient and very expressive.

The MAD dataset combines "stable" inorganic structures from the
`MC3D dataset <https://mc3d.materialscloud.org/>`_,
2D structures from the
`MC2D dataset <https://mc2d.materialscloud.org/>`_,
and molecular crystals from the
`ShiftML dataset <https://archive.materialscloud.org/record/2022.147>`_
with "Maximum Atomic Diversity" configurations, generated by distorting the composition
and structure of these stable templates. By doing so, PET-MAD achieves state-of-the-art
accuracy despite the MAD dataset containing fewer than 100k structures. The reference
DFT settings are highly converged, but limited to a PBEsol functional, so the accuracy
against experimental data depends on how good this level of theory is for a given
system. PET-MAD is introduced, and benchmarked for several challenging modeling tasks,
in `this preprint <https://arxiv.org/abs/2503.14118>`_.
"""

# %%
#
# We will finetune PET-MAD on a very simple
# dataset composed of 100 structres of ethanol
#

import subprocess
from collections import Counter

import ase.io
import numpy as np
from metatrain.pet import PET
from sklearn.linear_model import LinearRegression


if hasattr(__import__("builtins"), "get_ipython"):
    get_ipython().run_line_magic("matplotlib", "inline")  # noqa: F821


# %%
# Download the model
# ----------------------------------------
#
# We will download the model from
# `HuggingFace <https://huggingface.co/lab-cosmo/pet-mad>`_.
# You can do it in a bash script via the command:
#
# .. code-block:: bash
#
#  wget https://huggingface.co/lab-cosmo/pet-mad/resolve/main/models/pet-mad-latest.ckpt
#

url = "https://huggingface.co/lab-cosmo/pet-mad/resolve/main/models/pet-mad-latest.ckpt"
output_file = "pet-mad-latest.ckpt"

# Esegui wget
subprocess.run(["wget", url, "-O", output_file], check=True)

# %%
# Prepare the dataset
# --------------------
#
# Since the dataset we want to finetune on could be generate with different
# DFT parameters,
# such as pseudopotential or functional, we will correct the energy by a
# combination of compositional models.
# Following there are few helper functions.


def extract_atom_types(structures):
    """Extract unique atom types from a list of ASE Atoms objects."""
    atom_types = set()
    for atoms in structures:
        atom_types.update(atoms.get_chemical_symbols())
    return sorted(atom_types)


def get_rmse(y_true, y_pred):
    """Calculate the root mean square error (RMSE) between
    true and predicted values."""
    return np.sqrt(np.mean((y_true - y_pred) ** 2))


def prepare_features(traj):
    """Prepare feature matrix for linear regression."""
    compositions = [Counter(atoms.get_chemical_symbols()) for atoms in traj]
    species = sorted({element for comp in compositions for element in comp})
    X = np.array([[comp.get(s, 0) for s in species] for comp in compositions])
    return X, species


# Prepare the feature matrix (X) and target vector (y)
def prepare_features_and_target(traj, energies_DFT):
    """Prepare feature matrix and target vector for linear regression."""
    X, species = prepare_features(traj)
    y = np.array(energies_DFT)
    return X, y, species


def get_compositional_energy(atom, atom_energy):
    """Get the composition of the atoms object."""
    energy = 0
    for sp in atom_energy.keys():
        energy += atom_energy[sp] * atom.get_chemical_symbols().count(sp)
    return energy


def exctract_from_pet_mad(checkpoint="finetuning/pet-mad-latest.ckpt"):
    mdlpet = PET.load_checkpoint(checkpoint)
    vals = mdlpet.additive_models[0].weights["energy"].block().values
    specorder = [
        "H",
        "He",
        "Li",
        "Be",
        "B",
        "C",
        "N",
        "O",
        "F",
        "Ne",
        "Na",
        "Mg",
        "Al",
        "Si",
        "P",
        "S",
        "Cl",
        "Ar",
        "K",
        "Ca",
        "Sc",
        "Ti",
        "V",
        "Cr",
        "Mn",
        "Fe",
        "Co",
        "Ni",
        "Cu",
        "Zn",
        "Ga",
        "Ge",
        "As",
        "Se",
        "Br",
        "Kr",
        "Rb",
        "Sr",
        "Y",
        "Zr",
        "Nb",
        "Mo",
        "Tc",
        "Ru",
        "Rh",
        "Pd",
        "Ag",
        "Cd",
        "In",
        "Sn",
        "Sb",
        "Te",
        "I",
        "Xe",
        "Cs",
        "Ba",
        "La",
        "Ce",
        "Pr",
        "Nd",
        "Pm",
        "Sm",
        "Eu",
        "Gd",
        "Tb",
        "Dy",
        "Ho",
        "Er",
        "Tm",
        "Yb",
        "Lu",
        "Hf",
        "Ta",
        "W",
        "Re",
        "Os",
        "Ir",
        "Pt",
        "Au",
        "Hg",
        "Tl",
        "Pb",
        "Bi",
        "Po",
        "Rn",
    ]

    return {spec: vals[i] for i, spec in enumerate(specorder)}


def get_energy_men_atomic(traj, model, label="energy-men-atomic"):
    """Get the energy per atom for each trajectory."""
    energies_DFT = []
    for atoms in traj:
        energies_DFT.append(atoms.get_potential_energy())
    # Get the compositional energy for the DFT energies
    X, y, species = prepare_features_and_target(traj, energies_DFT)

    # Train a linear regression model
    modelLinear = LinearRegression()
    modelLinear.fit(X, y)

    # Display the coefficients
    coefficients = dict(zip(species, modelLinear.coef_))

    # Get the energy per atom of isolated atoms for the model

    ener = exctract_from_pet_mad(model)

    for i, atoms in enumerate(traj[:]):
        atoms.info[label] = (
            energies_DFT[i]
            - get_compositional_energy(atoms, coefficients)
            - modelLinear.intercept_
            + get_compositional_energy(atoms, ener)
        ).item()

    return traj


# %%
# Load the dataset
# ^^^^^^^^^^^^^^^^
# The dataset is composed of 100 structures of ethanol, with
# different DFT settings.
# Now we can load the dataset and correct the energies.
# then we can save the dataset to a file.

dataset = ase.io.read("data/ethanol_reduced_100.xyz", index=":", format="extxyz")

dataset_corrected = get_energy_men_atomic(
    dataset, model="pet-mad-latest.ckpt", label="energy-men-atomic"
)
ase.io.write("data/ethanol_corrected.xyz", dataset_corrected, format="extxyz")

# %%
# Start the finetuning
# ^^^^^^^^^^^^^^^^^^^^^
# we will use the `metatrain` package to finetune the model, and
# the input file in the `data` folder.
# Note that for our simple example the `training_set`, the
# `validation_set` and the `test_set` are the same, for real use cases
# you should use different sets.
# The input file is a `.yaml` file that contains the parameters for the
# finetuning.
# From command line you can run the finetuning with the following command:
# .. code-block:: bash
#
#   mtt train data/options.yaml -c pet-mad-latest.ckpt > train.log
#

subprocess.run(
    [
        "mtt",
        "train",
        "data/options.yaml",
        "-c",
        "pet-mad-latest.ckpt",
    ],
    check=True,
)
