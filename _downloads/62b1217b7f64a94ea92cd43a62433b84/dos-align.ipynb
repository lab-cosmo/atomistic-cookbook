{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# A ML model for the electron density of states\n:Authors: How Wei Bin [@HowWeiBin](https://github.com/HowWeiBin/)\n\nThis tutorial would go through the entire machine learning framework for the electronic\ndensity of states (DOS). It will cover the construction of the DOS and SOAP\ndescriptors from ase Atoms and eigenenergy results. A simple neural network will\nthen be constructed and the model parameters, along with the energy reference will be\noptimized during training. A total of three energy reference will be used, the average\nHartree potential, the Fermi level, and an optimized energy reference starting from\nthe Fermi level energy reference. The performance of each model is then compared.\n\nFirst, lets begin by importing the necessary packages and helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport zipfile\n\nimport ase\nimport ase.io\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport requests\nimport torch\nfrom featomic import SoapPowerSpectrum\nfrom scipy.interpolate import CubicHermiteSpline, interp1d\nfrom scipy.optimize import brentq\nfrom torch.utils.data import BatchSampler, DataLoader, Dataset, RandomSampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 0: Load Structures and Eigenenergies\n1) Downloading and Extracting Data\n2) Loading Data\n3) Find range of eigenenergies\n\nWe take a small subset of 104 structures in the Si dataset from `Bartok et al.,\n2018 <https://journals.aps.org/prx/abstract/10.1103/PhysRevX.8.041048>`.\nEach structure in the dataset contains two atoms.\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1) Downloading and Extracting Data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "filename = \"dataset.zip\"\nif not os.path.exists(filename):\n    url = \"https://github.com/HowWeiBin/datasets/archive/refs/tags/Silicon-Diamonds.zip\"\n    response = requests.get(url)\n    response.raise_for_status()\n    with open(filename, \"wb\") as f:\n        f.write(response.content)\n\nwith zipfile.ZipFile(\"./dataset.zip\", \"r\") as zip_ref:\n    zip_ref.extractall(\"./\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2) Loading Data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "structures = ase.io.read(\"./datasets-Silicon-Diamonds/diamonds.xyz\", \":\")\nn_structures = len(structures)\nn_atoms = torch.tensor([len(i) for i in structures])\neigenenergies = torch.load(\"./datasets-Silicon-Diamonds/diamond_energies.pt\")\nk_normalization = torch.tensor(\n    [len(i) for i in eigenenergies]\n)  # Calculates number of kpoints sampled per structure\nprint(f\"Total number of structures: {len(structures)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3) Find range of eigenenergies\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Process eigenenergies to flattened torch tensors\n\ntotal_eigenenergies = [torch.flatten(torch.tensor(i)) for i in eigenenergies]\n\n# Get lowest and highest value of eigenenergies to know the range of eigenenergies\n\nall_eigenenergies = torch.hstack(total_eigenenergies)\nminE = torch.min(all_eigenenergies)\nmaxE = torch.max(all_eigenenergies)\nprint(f\"The lowest eigenenergy in the dataset is {minE:.3}\")\nprint(f\"The highest eigenenergy in the dataset is {maxE:.3}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Constructing the DOS with different energy references\n1) Construct the DOS using the original reference\n2) Calculate the Fermi level from the DOS\n3) Build a set of eigenenergies, with the energy reference set to the fermi level\n4) Truncate the DOS energy window so that the DOS is well-defined at each point\n5) Construct the DOS in the truncated energy window under both references\n6) Construct Splines for the DOS to facilitate interpolation during model training\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1) Construct the DOS using the original reference\nThe DOS will first be constructed from the full set of eigenenergies to\ndetermine the Fermi level of each structure. The original reference is the\nAverage Hartree Potential in this example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# To ensure that all the eigenenergies are fully represented after\n# gaussian broadening, the energy axis of the DOS extends\n# 3eV wider than the range of values for the eigenenergies\nenergy_lower_bound = minE - 1.5\nenergy_upper_bound = maxE + 1.5\n\n# Gaussian Smearing for the eDOS, 0.3eV is the appropriate value for this dataset\n\nsigma = torch.tensor(0.3)\nenergy_interval = 0.05\n# energy axis, with a grid interval of 0.05 eV\n\nx_dos = torch.arange(energy_lower_bound, energy_upper_bound, energy_interval)\nprint(\n    f\"The energy axis ranges from {energy_lower_bound:.3} to \\\n{energy_upper_bound:.3}, consisting of {len(x_dos)} grid points\"\n)\n\n# normalization factor for each DOS, factor of 2 is included\n# because each eigenenergy can be occupied by 2 electrons\n\nnormalization = 2 * (\n    1 / torch.sqrt(2 * torch.tensor(np.pi) * sigma**2) / n_atoms / k_normalization\n)\n\ntotal_edos = []\n\nfor structure_eigenenergies in total_eigenenergies:\n    e_dos = torch.sum(\n        # Builds a gaussian on each eigenenergy\n        # and calculates the value on each grid point\n        torch.exp(-0.5 * ((x_dos - structure_eigenenergies.view(-1, 1)) / sigma) ** 2),\n        dim=0,\n    )\n    total_edos.append(e_dos)\n\ntotal_edos = torch.vstack(total_edos)\ntotal_edos = (total_edos.T * normalization).T\n\nprint(f\"The final shape of all the DOS in the dataset is: {list(total_edos.shape)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2) Calculate the Fermi level from the DOS\nNow we integration the DOS, and then use cubic interpolation and brentq\nto calculate the fermi level. Since only the 4 valence electrons in Silicon\nare represented in this energy range, we take the point where the DOS integrates\nto 4 as the fermi level.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fermi_levels = []\ntotal_i_edos = torch.cumulative_trapezoid(\n    total_edos, x_dos, axis=1\n)  # Integrate the DOS along the energy axis\nfor i in total_i_edos:\n    interpolated = interp1d(\n        x_dos[:-1], i - 4, kind=\"cubic\", copy=True, assume_sorted=True\n    )  # We use i-4 because Silicon has 4 electrons in this energy range\n    Ef = brentq(\n        interpolated, x_dos[0] + 0.1, x_dos[-1] - 0.1\n    )  # Fermi Level is the point where the (integrated DOS - 4) = 0\n    # 0.1 is added and subtracted to prevent brentq from going out of range\n    fermi_levels.append(Ef)\nfermi_levels = torch.tensor(fermi_levels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3) Build a set of eigenenergies, with the energy reference set to the fermi level\nUsing the fermi levels, we are now able to change the energy reference\nof the eigenenergies to the fermi level\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "total_eigenenergies_Ef = []\nfor index, energies in enumerate(total_eigenenergies):\n    total_eigenenergies_Ef.append(energies - fermi_levels[index])\n\nall_eigenenergies_Ef = torch.hstack(total_eigenenergies_Ef)\n\nminE_Ef = torch.min(all_eigenenergies_Ef)\nmaxE_Ef = torch.max(all_eigenenergies_Ef)\nprint(f\"The lowest eigenenergy using the fermi level energy reference is {minE_Ef:.3}\")\nprint(f\"The highest eigenenergy using the fermi level energy reference is {maxE_Ef:.3}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4) Truncate the DOS energy window so that the DOS is well-defined at each point\nWith the fermi levels, we can also truncate the energy window for DOS prediction.\nIn this example, we truncate the energy window such that it is 3eV above\nthe highest Fermi level in the dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For the Average Hartree Potential energy reference\nx_dos_H = torch.arange(minE - 1.5, max(fermi_levels) + 3, energy_interval)\n\n# For the Fermi Level Energy Reference, all the Fermi levels in the dataset is 0eV\nx_dos_Ef = torch.arange(minE_Ef - 1.5, 3, energy_interval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5) Construct the DOS in the truncated energy window under both references\nHere we construct 2 different targets where they differ in the energy reference\nchosen. These targets will then be treated as different datasets for the model\nto learn on.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For the Average Hartree Potential energy reference\n\ntotal_edos_H = []\n\nfor structure_eigenenergies_H in total_eigenenergies:\n    e_dos = torch.sum(\n        torch.exp(\n            -0.5 * ((x_dos_H - structure_eigenenergies_H.view(-1, 1)) / sigma) ** 2\n        ),\n        dim=0,\n    )\n    total_edos_H.append(e_dos)\n\ntotal_edos_H = torch.vstack(total_edos_H)\ntotal_edos_H = (total_edos_H.T * normalization).T\n\n\n# For the Fermi Level Energy Reference\n\ntotal_edos_Ef = []\n\nfor structure_eigenenergies_Ef in total_eigenenergies_Ef:\n    e_dos = torch.sum(\n        torch.exp(\n            -0.5 * ((x_dos_Ef - structure_eigenenergies_Ef.view(-1, 1)) / sigma) ** 2\n        ),\n        dim=0,\n    )\n    total_edos_Ef.append(e_dos)\n\ntotal_edos_Ef = torch.vstack(total_edos_Ef)\ntotal_edos_Ef = (total_edos_Ef.T * normalization).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6) Construct Splines for the DOS to facilitate interpolation during model training\nBuilding Cubic Hermite Splines on the DOS on the truncated energy window\nto facilitate interpolation during training. Cubic Hermite Splines takes\nin information on the value and derivative of a function at a point to build splines.\nThus, we will have to compute both the value and derivative at each spline position\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Functions to compute the value and derivative of the DOS at each energy value, x\ndef edos_value(x, eigenenergies, normalization):\n    e_dos_E = (\n        torch.sum(\n            torch.exp(-0.5 * ((x - eigenenergies.view(-1, 1)) / sigma) ** 2), dim=0\n        )\n        * normalization\n    )\n\n    return e_dos_E\n\n\ndef edos_derivative(x, eigenenergies, normalization):\n    dfn_dos_E = (\n        torch.sum(\n            torch.exp(-0.5 * ((x - eigenenergies.view(-1, 1)) / sigma) ** 2)\n            * (-1 * ((x - eigenenergies.view(-1, 1)) / sigma) ** 2),\n            dim=0,\n        )\n        * normalization\n    )\n\n    return dfn_dos_E\n\n\ntotal_splines_H = []\n# the splines have a higher energy range in case the shift is high\nspline_positions_H = torch.arange(minE - 2, max(fermi_levels) + 6, energy_interval)\n\nfor index, structure_eigenenergies_H in enumerate(total_eigenenergies):\n    e_dos_H = edos_value(\n        spline_positions_H, structure_eigenenergies_H, normalization[index]\n    )\n    e_dos_H_grad = edos_derivative(\n        spline_positions_H, structure_eigenenergies_H, normalization[index]\n    )\n    spliner = CubicHermiteSpline(spline_positions_H, e_dos_H, e_dos_H_grad)\n    total_splines_H.append(torch.tensor(spliner.c))\n\ntotal_splines_H = torch.stack(total_splines_H)\n\ntotal_splines_Ef = []\nspline_positions_Ef = torch.arange(minE_Ef - 2, 6, energy_interval)\n\nfor index, structure_eigenenergies_Ef in enumerate(total_eigenenergies_Ef):\n    e_dos_Ef = edos_value(\n        spline_positions_Ef, structure_eigenenergies_Ef, normalization[index]\n    )\n    e_dos_Ef_grad = edos_derivative(\n        spline_positions_Ef, structure_eigenenergies_Ef, normalization[index]\n    )\n    spliner = CubicHermiteSpline(spline_positions_Ef, e_dos_Ef, e_dos_Ef_grad)\n    total_splines_Ef.append(torch.tensor(spliner.c))\n\ntotal_splines_Ef = torch.stack(total_splines_Ef)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have stored the splines coefficients (spliner.c) as torch tensors,\nas such as we will need to write a function to evaluate the DOS from\nthe splines positions and coefficients.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def evaluate_spline(spline_coefs, spline_positions, x):\n    \"\"\"\n    spline_coefs: shape of (n x 4 x spline_positions)\n\n    return value: shape of (n x x)\n\n    x : shape of (n x n_points)\n    \"\"\"\n    interval = torch.round(\n        spline_positions[1] - spline_positions[0], decimals=4\n    )  # get spline grid intervals\n    x = torch.clamp(\n        x, min=spline_positions[0], max=spline_positions[-1] - 0.0005\n    )  # restrict x to fall within the spline interval\n    # 0.0005 is subtracted to combat errors arising from precision\n    indexes = torch.floor(\n        (x - spline_positions[0]) / interval\n    ).long()  # Obtain the index for the appropriate spline coefficients\n    expanded_index = indexes.unsqueeze(dim=1).expand(-1, 4, -1)\n    x_1 = x - spline_positions[indexes]\n    x_2 = x_1 * x_1\n    x_3 = x_2 * x_1\n    x_0 = torch.ones_like(x_1)\n    x_powers = torch.stack([x_3, x_2, x_1, x_0]).permute(1, 0, 2)\n    value = torch.sum(\n        torch.mul(x_powers, torch.gather(spline_coefs, 2, expanded_index)), axis=1\n    )\n\n    return value  # returns the value of the DOS at the energy positions, x."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets look at the accuracy of the splines.\nTest 1: Ability to reproduce the correct values at the default x_dos positions\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "shifts = torch.zeros(n_structures)\nx_dos_splines = x_dos_H + shifts.view(-1, 1)\nspline_dos_H = evaluate_spline(total_splines_H, spline_positions_H, x_dos_splines)\n\nplt.plot(x_dos_H, total_edos_H[0], color=\"red\", label=\"True DOS\")\nplt.plot(x_dos_H, spline_dos_H[0], color=\"blue\", linestyle=\"--\", label=\"Spline DOS\")\nplt.legend()\nplt.xlabel(\"Energy [eV]\")\nplt.ylabel(\"DOS\")\nprint(\"Both lines lie on each other\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test 2: Ability to reproduce the correct values at the shifted x_dos positions\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "shifts = torch.zeros(n_structures) + 0.3\nx_dos_splines = x_dos_Ef + shifts.view(-1, 1)\nspline_dos_Ef = evaluate_spline(total_splines_Ef, spline_positions_Ef, x_dos_splines)\n\nplt.plot(x_dos_Ef, total_edos_Ef[0], color=\"red\", label=\"True DOS\")\nplt.plot(x_dos_Ef, spline_dos_Ef[0], color=\"blue\", linestyle=\"--\", label=\"Spline DOS\")\nplt.legend()\nplt.xlabel(\"Energy [eV]\")\nplt.ylabel(\"DOS\")\nprint(\"Both spectras look very similar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Compute SOAP power spectrum for the dataset\n\nWe first define the hyperparameters to compute the\nSOAP power spectrum using featomic.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "HYPER_PARAMETERS = {\n    \"cutoff\": {\"radius\": 4.0, \"smoothing\": {\"type\": \"Step\"}},\n    \"density\": {\n        \"type\": \"Gaussian\",\n        \"width\": 0.45,\n        \"scaling\": {\"type\": \"Willatt2018\", \"exponent\": 5, \"rate\": 1, \"scale\": 3.0},\n    },\n    \"basis\": {\n        \"type\": \"TensorProduct\",\n        \"max_angular\": 6,\n        \"radial\": {\"type\": \"Gto\", \"max_radial\": 7},\n    },\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We feed the Hyperparameters into featomic to compute the SOAP Power spectrum\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "calculator = SoapPowerSpectrum(**HYPER_PARAMETERS)\nR_total_soap = calculator.compute(structures)\n# Transform the tensormap to a single block containing a dense representation\nR_total_soap.keys_to_samples(\"center_type\")\nR_total_soap.keys_to_properties([\"neighbor_1_type\", \"neighbor_2_type\"])\n\n# Now we extract the data tensor from the single block\ntotal_atom_soap = []\nfor structure_i in range(n_structures):\n    a_i = R_total_soap.block(0).samples[\"system\"] == structure_i\n    total_atom_soap.append(torch.tensor(R_total_soap.block(0).values[a_i, :]))\n\ntotal_soap = torch.stack(total_atom_soap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Building a Simple MLP Model\n\n1) Split the data into Training, Validation and Test\n2) Define the dataloader and the Model Architecture\n3) Define relevant loss functions for training and inference\n4) Define the training loop\n5) Evaluate the model\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1) Split the data into Training, Validation and Test\nWe will first split the data in a 7:1:2 manner, corresponding to train, val and test.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\ntrain_index = np.arange(n_structures)\nnp.random.shuffle(train_index)\ntest_mark = int(0.8 * n_structures)\nval_mark = int(0.7 * n_structures)\ntest_index = train_index[test_mark:]\nval_index = train_index[val_mark:test_mark]\ntrain_index = train_index[:val_mark]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2) Define the dataloader and the Model Architecture\nWe will now build a dataloader and dataset to facilitate training the model batchwise\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def generate_atomstructure_index(n_atoms_per_structure):\n    \"\"\"Generate a sequence of indices for each atom in the structure.\n    The indices correspond to index of the structure that the atom belongs to\n\n    Args:\n        n_atoms_per_structure ([array]):\n        [Array containing the number of atoms each structure contains]\n\n    Return s:\n        [tensor]: [Total index, matching atoms to structure]\n    \"\"\"\n    # n_structures = len(n_atoms_per_structure)\n    total_index = []\n    for i, atoms in enumerate(n_atoms_per_structure):\n        indiv_index = torch.zeros(atoms) + i\n        total_index.append(indiv_index)\n    total_index = torch.hstack(total_index)\n    return total_index.long()\n\n\nclass AtomicDataset(Dataset):\n    def __init__(self, features, n_atoms_per_structure):\n        self.features = features\n        self.n_structures = len(n_atoms_per_structure)\n        self.n_atoms_per_structure = n_atoms_per_structure\n        self.index = generate_atomstructure_index(self.n_atoms_per_structure)\n        assert torch.sum(n_atoms_per_structure) == len(features)\n\n    def __len__(self):\n        return self.n_structures\n\n    def __getitem__(self, idx):\n        if isinstance(idx, list):  # If a list of indexes is given\n            feature_indexes = []\n\n            for i in idx:\n                feature_indexes.append((self.index == i).nonzero(as_tuple=True)[0])\n\n            feature_indexes = torch.hstack(feature_indexes)\n\n            return (\n                self.features[feature_indexes],\n                idx,\n                generate_atomstructure_index(self.n_atoms_per_structure[idx]),\n            )\n\n        else:\n            feature_indexes = (self.index == idx).nonzero(as_tuple=True)[0]\n            return (\n                self.features[feature_indexes],\n                idx,\n                self.n_atoms_per_structure[idx],\n            )\n\n\ndef collate(\n    batch,\n):  # Defines how to collate the outputs of the __getitem__ function at each batch\n    for x, idx, index in batch:\n        return (x, idx, index)\n\n\nx_train = torch.flatten(total_soap[train_index], 0, 1).float()\ntotal_atomic_soaps = torch.vstack(total_atom_soap).float()\ntrain_features = AtomicDataset(x_train, n_atoms[train_index])\nfull_atomstructure_index = generate_atomstructure_index(n_atoms)\n# Will be required later to collate atomic predictions into structural predictions\n\n# Build a Dataloader that samples from the AtomicDataset in random batches\nSampler = RandomSampler(train_features)\nBSampler = BatchSampler(Sampler, batch_size=32, drop_last=False)\ntraindata_loader = DataLoader(train_features, sampler=BSampler, collate_fn=collate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now define a simple three layer MLP model, consisting of three layers.\nThe align keyword is used to indicate that the energy reference will be optimized\nduring training. The alignment parameter refers to the adjustments made to the\ninitial energy referenced and will be initialized as zeros.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class SOAP_NN(torch.nn.Module):\n    def __init__(self, input_dims, L1, n_train, target_dims, align):\n        super(SOAP_NN, self).__init__()\n        self.target_dims = target_dims\n        self.fc1 = torch.nn.Linear(input_dims, L1)\n        self.fc2 = torch.nn.Linear(L1, target_dims)\n        self.silu = torch.nn.SiLU()\n        self.align = align\n        if align:\n            initial_alignment = torch.zeros(n_train)\n            self.alignment = torch.nn.parameter.Parameter(initial_alignment)\n\n    def forward(self, x):\n        result = self.fc1(x)\n        result = self.silu(result)\n        result = self.fc2(result)\n        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use a small network architecture, whereby the input layer corresponds\nto the size of the SOAP features, 448, the intermediate layer corresponds to\na tenth size of the input layer and the final layer corresponds\nto the number of outputs.\n\nAs a shorthand, the model that optimizes the energy reference during\ntraining will be called the alignment model\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_outputs_H = len(x_dos_H)\nn_outputs_Ef = len(x_dos_Ef)\n\n\nModel_H = SOAP_NN(\n    x_train.shape[1],\n    x_train.shape[1] // 10,\n    len(train_index),\n    n_outputs_H,\n    align=False,\n)\nModel_Ef = SOAP_NN(\n    x_train.shape[1],\n    x_train.shape[1] // 10,\n    len(train_index),\n    n_outputs_Ef,\n    align=False,\n)\nModel_Align = SOAP_NN(\n    x_train.shape[1],\n    x_train.shape[1] // 10,\n    len(train_index),\n    n_outputs_Ef,\n    align=True,\n)\n# The alignment model takes the fermi level energy reference as the starting point"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3) Define relevant loss functions for training and inference\nWe will now define some loss functions that will be useful when we implement\nthe model training loop later and during model evaluation on the test set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def t_get_mse(a, b, xdos):\n    \"\"\"Compute mean error between two Density of States.\n    The mean error is integrated across the entire energy grid\n    to provide a single value to characterize the error\n\n    Args:\n        a ([tensor]): [Predicted DOS]\n        b ([tensor]): [True DOS]\n        xdos ([tensor], optional): [Energy axis of DOS]\n\n    Returns:\n        [float]: [MSE]\n    \"\"\"\n    if len(a.size()) > 1:\n        mse = (torch.trapezoid((a - b) ** 2, xdos, axis=1)).mean()\n    else:\n        mse = (torch.trapezoid((a - b) ** 2, xdos, axis=0)).mean()\n    return mse\n\n\ndef t_get_rmse(a, b, xdos):\n    \"\"\"Compute root mean squared error between two Density of States .\n\n    Args:\n        a ([tensor]): [Predicted DOS]\n        b ([tensor]): [True DOS]\n        xdos ([tensor], optional): [Energy axis of DOS]\n\n    Raises:\n        ValueError: [Occurs if tensor shapes are mismatched]\n\n    Returns:\n        [float]: [RMSE or %RMSE]\n    \"\"\"\n\n    if len(a.size()) > 1:\n        rmse = torch.sqrt((torch.trapezoid((a - b) ** 2, xdos, axis=1)).mean())\n    else:\n        rmse = torch.sqrt((torch.trapezoid((a - b) ** 2, xdos, axis=0)).mean())\n    return rmse\n\n\ndef Opt_RMSE_spline(y_pred, xdos, target_splines, spline_positions, n_epochs):\n    \"\"\"Evaluates RMSE on the optimal shift of energy axis.\n    The optimal shift is found via gradient descent after a gridsearch is performed.\n\n    Args:\n        y_pred ([tensor]): [Prediction/s of DOS]\n        xdos ([tensor]): [Energy axis]\n        target_splines ([tensor]): [Contains spline coefficients]\n        spline_positions ([tensor]): [Contains spline positions]\n        n_epochs ([int]): [Number of epochs to run for Gradient Descent]\n\n    Returns:\n        [rmse([float]), optimal_shift[tensor]]:\n        [RMSE on optimal shift, the optimal shift itself]\n\n    \"\"\"\n    optim_search_mse = []\n    offsets = torch.arange(-2, 2, 0.1)\n    # Grid-search is first done to reduce number of epochs needed for\n    # gradient descent, typically 50 epochs will be sufficient\n    # if searching within 0.1\n    with torch.no_grad():\n        for offset in offsets:\n            shifts = torch.zeros(y_pred.shape[0]) + offset\n            shifted_target = evaluate_spline(\n                target_splines, spline_positions, xdos + shifts.view(-1, 1)\n            )\n            loss_i = ((y_pred - shifted_target) ** 2).mean(dim=1)\n            optim_search_mse.append(loss_i)\n        optim_search_mse = torch.vstack(optim_search_mse)\n        min_index = torch.argmin(optim_search_mse, dim=0)\n        optimal_offset = offsets[min_index]\n\n    offset = optimal_offset\n\n    shifts = torch.nn.parameter.Parameter(offset.float())\n    opt_adam = torch.optim.Adam([shifts], lr=1e-2)\n    best_error = torch.zeros(len(shifts)) + 100\n    best_shifts = shifts.clone()\n    for _ in range(n_epochs):\n        shifted_target = evaluate_spline(\n            target_splines, spline_positions, xdos + shifts.view(-1, 1)\n        ).detach()\n\n        def closure():\n            opt_adam.zero_grad()\n            shifted_target = evaluate_spline(\n                target_splines, spline_positions, xdos + shifts.view(-1, 1)\n            )\n            loss_i = ((y_pred - shifted_target) ** 2).mean()\n            loss_i.backward(gradient=torch.tensor(1), inputs=shifts)\n            return loss_i\n\n        opt_adam.step(closure)\n\n        with torch.no_grad():\n            each_loss = ((y_pred - shifted_target) ** 2).mean(dim=1).float()\n            index = each_loss < best_error\n            best_error[index] = each_loss[index].clone()\n            best_shifts[index] = shifts[index].clone()\n\n    # Evaluate\n\n    optimal_shift = best_shifts\n    shifted_target = evaluate_spline(\n        target_splines, spline_positions, xdos + optimal_shift.view(-1, 1)\n    )\n    rmse = t_get_rmse(y_pred, shifted_target, xdos)\n    return rmse, optimal_shift"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4) Define the training loop\n\nWe will now define the model training loop, for simplicity we will only\ntrain each model for a fixed number of epochs, learning rate, and batch_size.\nThe training and validation error at each epoch will be saved.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train_model(model_to_train, fixed_DOS, structure_splines, spline_positions, x_dos):\n    \"\"\"Trains a model for 500 epochs\n\n    Args:\n        model_to_train ([torch.nn.Module]): [ML Model]\n        fixed_DOS ([tensor]): [Contains the DOS at a fixed energy reference,\n        useful for models that don't optimize the energy reference]\n        structure_splines ([tensor]): [Contains spline coefficients]\n        spline_positions ([tensor]): [Contains spline positions]\n        x_dos ([tensor]): [Energy axis for the prediction]\n\n    Returns:\n        [train_loss_history([tensor]),\n        val_loss_history[tensor],\n        structure_results[tensor]]:\n\n        [Respective loss histories and final structure predictions]\n    \"\"\"\n    lr = 1e-2\n    n_epochs = 500\n\n    opt = torch.optim.Adam(model_to_train.parameters(), lr=lr)\n\n    train_loss_history = []\n    val_loss_history = []\n\n    for _epoch in range(n_epochs):\n        for x_data, idx, index in traindata_loader:\n            opt.zero_grad()\n            predictions = model_to_train.forward(x_data)\n            structure_results = torch.zeros([len(idx), model_to_train.target_dims])\n            # Sum atomic predictions in each structure\n            structure_results = structure_results.index_add_(\n                0, index, predictions\n            ) / n_atoms[train_index[idx]].view(-1, 1)\n            if model_to_train.align:\n                alignment = model_to_train.alignment\n                alignment = alignment - torch.mean(alignment)\n                # Enforce that the alignments have a mean of zero since a constant\n                # value across the dataset is meaningless when optimizing the\n                # relative energy reference\n                target = evaluate_spline(\n                    structure_splines[train_index[idx]],\n                    spline_positions,\n                    x_dos + alignment[idx].view(-1, 1),\n                )  # Shifts the target based on the alignment value\n                pred_loss = t_get_mse(structure_results, target, x_dos)\n                pred_loss.backward()\n            else:\n                pred_loss = t_get_mse(\n                    structure_results, fixed_DOS[train_index[idx]], x_dos\n                )\n                pred_loss.backward()\n\n            opt.step()\n        with torch.no_grad():\n            all_pred = model_to_train.forward(total_atomic_soaps.float())\n            structure_results = torch.zeros([n_structures, model_to_train.target_dims])\n            structure_results = structure_results.index_add_(\n                0, full_atomstructure_index, all_pred\n            ) / (n_atoms).view(-1, 1)\n            if model_to_train.align:\n                # Evaluate model on optimal shift as there is no information\n                # regarding the shift from the fermi level energy reference\n                # during inference\n\n                alignment = model_to_train.alignment\n                alignment = alignment - torch.mean(alignment)\n                target = evaluate_spline(\n                    structure_splines[train_index],\n                    spline_positions,\n                    x_dos + alignment.view(-1, 1),\n                )\n\n                train_loss = t_get_rmse(structure_results[train_index], target, x_dos)\n                val_loss, val_shifts = Opt_RMSE_spline(\n                    structure_results[val_index],\n                    x_dos,\n                    structure_splines[val_index],\n                    spline_positions,\n                    50,\n                )\n\n            else:\n                train_loss = t_get_rmse(\n                    structure_results[train_index], fixed_DOS[train_index], x_dos\n                )\n                val_loss = t_get_rmse(\n                    structure_results[val_index], fixed_DOS[val_index], x_dos\n                )\n\n            train_loss_history.append(train_loss)\n            val_loss_history.append(val_loss)\n    train_loss_history = torch.tensor(train_loss_history)\n    val_loss_history = torch.tensor(val_loss_history)\n    return (\n        train_loss_history,\n        val_loss_history,\n        structure_results,\n    )\n    # returns the loss history and the final set of predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "H_trainloss, H_valloss, H_predictions = train_model(\n    Model_H, total_edos_H, total_splines_H, spline_positions_H, x_dos_H\n)\n\nEf_trainloss, Ef_valloss, Ef_predictions = train_model(\n    Model_Ef, total_edos_Ef, total_splines_Ef, spline_positions_Ef, x_dos_Ef\n)\n\nAlign_trainloss, Align_valloss, Align_predictions = train_model(\n    Model_Align, total_edos_Ef, total_splines_Ef, spline_positions_Ef, x_dos_Ef\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets plot the train loss histories to compare their learning behaviour\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "epochs = np.arange(500)\n\n\nplt.plot(epochs, H_trainloss, color=\"red\", label=\"Avg Hartree Potential\")\nplt.plot(epochs, Ef_trainloss, color=\"blue\", label=\"Fermi Level\")\nplt.plot(epochs, Align_trainloss, color=\"green\", label=\"Optimized Reference\")\nplt.legend()\nplt.yscale(value=\"log\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"RMSE\")\nplt.title(\"Train Loss vs Epoch\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets plot the val loss histories to compare their learning behaviour\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.plot(epochs, H_valloss, color=\"red\", label=\"Avg Hartree Potential\")\nplt.plot(epochs, Ef_valloss, color=\"blue\", label=\"Fermi Level\")\nplt.plot(epochs, Align_valloss, color=\"green\", label=\"Optimized Reference\")\nplt.legend()\nplt.yscale(value=\"log\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"RMSE\")\nplt.title(\"Validation Loss vs Epoch\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5) Evaluate the model\nWe will now evaluate the model performance on the test set\nbased on the model predictions we obtained previously\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "H_testloss = Opt_RMSE_spline(\n    H_predictions[test_index],\n    x_dos_H,\n    total_splines_H[test_index],\n    spline_positions_H,\n    200,\n)  # We use 200 epochs just so it the error a little bit more converged\nEf_testloss = Opt_RMSE_spline(\n    Ef_predictions[test_index],\n    x_dos_Ef,\n    total_splines_Ef[test_index],\n    spline_positions_Ef,\n    200,\n)  # We use 200 epochs just so it the error a little bit more converged\nAlign_testloss = Opt_RMSE_spline(\n    Align_predictions[test_index],\n    x_dos_Ef,\n    total_splines_Ef[test_index],\n    spline_positions_Ef,\n    200,\n)  # We use 200 epochs just so it the error a little bit more converged\n\nprint(f\"Test RMSE for average Hartree Potential: {H_testloss[0].item():.3}\")\nprint(f\"Test RMSE for Fermi Level: {Ef_testloss[0].item():.3}\")\nprint(f\"Test RMSE for Optimized Reference: {Align_testloss[0].item():.3}\")\n\nprint(\n    \"The difference in effectiveness between the Optimized Reference \\\nand the Fermi Level will increase with more epochs\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot Training DOSes at different energy reference to visualize\nthe impact of the energy reference. From the plots we can see\nthat the optimized energy reference has better alignment of\ncommon spectral patterns across the dataset\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAverage Hartree Energy Reference\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for i in total_edos_H[train_index]:\n    plt.plot(x_dos_H, i, color=\"C0\", alpha=0.6)\nplt.title(\"Energy Reference - Average Hartree Potential\")\nplt.xlabel(\"Energy [eV]\")\nplt.ylabel(\"DOS\")\nplt.show()\nprint(\"The DOSes, despite looking similar, are offset along the energy axis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fermi Level Energy Reference\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for i in total_edos_Ef[train_index]:\n    plt.plot(x_dos_Ef, i, color=\"C0\", alpha=0.6)\nplt.title(\"Energy Reference - Fermi Level\")\nplt.xlabel(\"Energy [eV]\")\nplt.ylabel(\"DOS\")\nplt.show()\n\nprint(\"It is better aligned but still quite some offset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimized Energy Reference\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "shifts = Model_Align.alignment.detach()\nshifts = shifts - torch.mean(shifts)\nx_dos_splines = x_dos_Ef + shifts.view(-1, 1)\ntotal_edos_align = evaluate_spline(\n    total_splines_Ef[train_index], spline_positions_Ef, x_dos_splines\n)\n\nfor i in total_edos_align:\n    plt.plot(x_dos_Ef, i, color=\"C0\", alpha=0.6)\nplt.title(\"Energy Reference - Optimized\")\nplt.xlabel(\"Energy [eV]\")\nplt.ylabel(\"DOS\")\nplt.show()\nprint(\"The DOS alignment is better under the optimized energy reference\")\nprint(\"The difference will increase with more training epochs\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}