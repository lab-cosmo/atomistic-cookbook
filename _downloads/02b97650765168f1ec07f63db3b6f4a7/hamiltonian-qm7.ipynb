{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Hamiltonian Learning for Molecules with Indirect Targets\n\n:Authors: Divya Suman [@DivyaSuman14](https://github.com/DivyaSuman14)_,\n          Hanna Tuerk [@HannaTuerk](https://github.com/HannaTuerk)_\n\nThis tutorial introduces a machine learning (ML) framework\nthat predicts Hamiltonians for molecular systems. Another\none of our [cookbook examples](https://atomistic-cookbook.org/examples/periodic-hamiltonian/periodic-hamiltonian.html)_\ndemonstrates an ML model that predicts real-space Hamiltonians\nfor periodic systems. While we use the same model here to predict\na molecular Hamiltonians, we further finetune these models to\noptimise predictions of different quantum mechanical (QM)\nproperties of interest, thereby treating the Hamiltonian\npredictions as an intermediate component of the ML\nframework. More details on this hybrid or indirect learning\nframework can be found in [ACS Cent. Sci. 2024, 10, 637\u2212648.](https://pubs.acs.org/doi/full/10.1021/acscentsci.3c01480)\nand our preprint [arXiv:2504.01187](https://doi.org/10.48550/arXiv.2504.01187).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Within a Hamiltonian learning framework, one could chose to learn\na target that corresponds to the matrix representation for an\nexisting electronic structure method, but in a finite AO basis,\nsuch a representation will lead to a finite basis set error.\nThe parity plot below illustrates this error by showing the\ndiscrepancy between molecular orbital (MO) energies of an ethane\nmolecule obtained from a self-consistent calculation on a minimal\nSTO-3G basis and the larger def2-TZVP basis, especially for the\nhigh energy, unoccupied MOs.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. figure:: minimal_vs_lb.png\n   :alt: Parity plot comparing the MO energies of ethane from a\n          DFT calculation with the STO-3G and the def2-TZVP basis.\n   :width: 600px\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The choice of basis set plays a crucial role in determining\nthe accuracy of the observables derived from the predicted\nHamiltonian. Although the larger basis sets generally provide\nmore reliable results, the computational cost to compute the electronic\nstructure in a larger basis or to train such a\nmodel is notably higher compared to a smaller basis.\nUsing the indirect learning framework, one\ncould instead learn a reduced effective Hamiltonian that reproduces\ncalculations from a much larger basis while using a significantly\nsimpler and smaller model consistent with a smaller basis.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first show an example where we predict the reduced effective\nHamiltonians for a homogenous dataset of ethane molecule while\ntargeting the MO energies of the def2-TZVP basis. In a second example\nwe will then target multiple properties for a organic molecule dataset,\nsimilar to our results described in our preprint\n[arXiv:2504.01187](https://doi.org/10.48550/arXiv.2504.01187).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Example of Learning MO Energies for Ethane\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Python Environment and Used Packages\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start by creating a virtual environment and installing\nall necessary packages. The required packages are provided\nin the environment.yml file that can be dowloaded at the end.\nWe can then import the necessary packages.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nfrom zipfile import ZipFile\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport requests\nimport torch\nfrom ase.units import Hartree\nfrom IPython.utils import io\nfrom mlelec.features.acdc import compute_features_for_target\nfrom mlelec.targets import drop_zero_blocks  # noqa: F401\nfrom mlelec.utils.plot_utils import plot_losses\n\n\nos.environ[\"PYSCFAD_BACKEND\"] = \"torch\"\nimport mlelec.metrics as mlmetrics  # noqa: E402\nfrom mlelec.data.dataset import MLDataset, MoleculeDataset, get_dataloader  # noqa: E402\nfrom mlelec.models.linear import LinearTargetModel  # noqa: E402\nfrom mlelec.train import Trainer  # noqa: E402\nfrom mlelec.utils.property_utils import (  # noqa: E402, F401\n    compute_dipole_moment,\n    compute_eigvals,\n    compute_polarisability,\n    instantiate_mf,\n)\n\n\ntorch.set_default_dtype(torch.float64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Parameters for Training\nBefore we begin our training we can decide on a set the parameters,\nincluding the dataset set size, splitting fractions, the batch size,\nlearning rate, number of epochs, and the early stop criterion in case of\nearly stopping.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "NUM_FRAMES = 100\nBATCH_SIZE = 4\nNUM_EPOCHS = 100\nSHUFFLE_SEED = 42\nTRAIN_FRAC = 0.7\nTEST_FRAC = 0.1\nVALIDATION_FRAC = 0.2\nEARLY_STOP_CRITERION = 20\nVERBOSE = 10\nDUMP_HIST = 50\nLR = 1e-1\nVAL_INTERVAL = 1\nDEVICE = \"cpu\"\n\nORTHOGONAL = True  # set to 'FALSE' if working in the non-orthogonal basis\nFOLDER_NAME = \"output/ethane_eva\"\nNOISE = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Folders and Save Parameters\nWe can save the parameters we just defined for our reference later.\nFor this, we create a folder (defined as FOLDER_NAME above)\nin which all parameters\nand the generated data for this example are stored.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "os.makedirs(FOLDER_NAME, exist_ok=True)\nos.makedirs(f\"{FOLDER_NAME}/model_output\", exist_ok=True)\n\n\ndef save_parameters(file_path, **params):\n    with open(file_path, \"w\") as file:\n        for key, value in params.items():\n            file.write(f\"{key}: {value}\\n\")\n\n\n# Call the function with your parameters\nsave_parameters(\n    f\"{FOLDER_NAME}/parameters.txt\",\n    NUM_FRAMES=NUM_FRAMES,\n    BATCH_SIZE=BATCH_SIZE,\n    NUM_EPOCHS=NUM_EPOCHS,\n    SHUFFLE_SEED=SHUFFLE_SEED,\n    TRAIN_FRAC=TRAIN_FRAC,\n    TEST_FRAC=TEST_FRAC,\n    VALIDATION_FRAC=VALIDATION_FRAC,\n    LR=LR,\n    VAL_INTERVAL=VAL_INTERVAL,\n    DEVICE=DEVICE,\n    ORTHOGONAL=ORTHOGONAL,\n    FOLDER_NAME=FOLDER_NAME,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate Reference Data\n\nIn principle one can generate the training data of reference\nHamiltonians from a given set of structures, using any\nelectronic structure code. Here we provide a pre-computed,\nhomogenous dataset that contains 100 different\nconfigurations of ethane molecule. For all structures, we\nperformed Kohn-Sham density functional theory (DFT)\ncalculations with [PySCF](https://github.com/pyscf/pyscf),\nusing the B3LYP functional. For each molecular geometry, we\ncomputed the Fock and overlap matrices along with other\nmolecular properties of interest, using both STO-3G and def2-TZVP\nbasis sets.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Download the Dataset from Zenodo\nWe first download the data for the two examples from Zenodo\nand unzip the downloaded datafile.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"hamiltonian-qm7-data\"):\n    url = r\"https://zenodo.org/records/15524259/files/hamiltonian-qm7-data.zip\"\n    response = requests.get(url)\n    response.raise_for_status()\n    with open(\"hamiltonian-qm7-data.zip\", \"wb\") as f:\n        f.write(response.content)\n\n    with ZipFile(\"hamiltonian-qm7-data.zip\", \"r\") as zObject:\n        zObject.extractall(path=\".\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepare the Dataset for ML Training\n\nIn this section, we will prepare the dataset required to\ntrain our machine learning model using the ``MoleculeDataset``\nand ``MLDataset`` classes. These classes help format and store\nthe DFT data in a way compatible with our ML package,\n[mlelec](https://github.com/curiosity54/mlelec/tree/qm7).\nIn this section we initialise the ``MoleculeDataset`` where\nwe specify the molecule name, file paths and the desired targets\nand auxillary data to be used for training for the minimal\n(STO-3G), as well as a larger basis (lb, def2-TZVP).\nOnce the molecular data is prepared, we wrap it into an\n``MLDataset`` instance. This class structures the dataset\ninto a format that is optimal for ML the Hamiltonians. The\nHamiltonian matrix elements depend on specific pairs of\norbitals involved in the interaction. When these orbitals\nare centered on atoms, as is the case for localized AO\nbases, the Hamiltonian matrix elements can be viewed\nas objects labeled by pairs of atoms, as well as multiple\nquantum numbers, namely the radial (`n`) and the angular\n(`l`, `m`) quantum numbers characterizing each AO. These\nangular functions are typically chosen to be real spherical\nharmonics, and determine the equivariant behavior of the\nmatrix elements under rotations and inversions. ``MLDataset``\nleverages this equivariant structure of the Hamiltonians,\nwhich is discussed in further detail in the [Periodic Hamiltonian Model Example](https://atomistic-cookbook.org/examples/periodic-hamiltonian/periodic-hamiltonian.html)_\n. Finally, we split the loaded dataset into training,\nvalidation and test datasets using ``_split_indices``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "molecule_data = MoleculeDataset(\n    mol_name=\"ethane\",\n    use_precomputed=False,\n    path=\"hamiltonian-qm7-data/ethane\",\n    aux_path=\"hamiltonian-qm7-data/ethane/sto-3g\",\n    frame_slice=slice(0, NUM_FRAMES),\n    device=DEVICE,\n    aux=[\"overlap\", \"orbitals\"],\n    lb_aux=[\"overlap\", \"orbitals\"],\n    target=[\"fock\", \"eva\"],\n    lb_target=[\"fock\", \"eva\"],\n)\n\nml_data = MLDataset(\n    molecule_data=molecule_data,\n    device=DEVICE,\n    model_strategy=\"coupled\",\n    shuffle=True,\n    shuffle_seed=SHUFFLE_SEED,\n    orthogonal=ORTHOGONAL,\n)\n\nml_data._split_indices(\n    train_frac=TRAIN_FRAC, val_frac=VALIDATION_FRAC, test_frac=TEST_FRAC\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Computing Features that can Learn Hamiltonian Targets\n\nAs discussed above, the Hamiltonian matrix elements\nare dependent on single atom centers and two centers\nfor pairwise interactions.\nTo address this, we extend the equivariant SOAP-based\nfeatures for the atom-centered desciptors\nto a descriptor capable of describing multiple atomic centers and\ntheir connectivities, giving rise to the equivariant pairwise descriptor\nwhich simultaneously characterizes the environments for pairs of atoms\nin a given system. Our [Periodic Hamiltonian Model Example](https://atomistic-cookbook.org/examples/periodic-hamiltonian/periodic-hamiltonian.html)_\ndiscusses the construction of these descriptors in greater detail.\nTo construct these atom- and pair-centered features we use our\nin house library [featomic](https://github.com/metatensor/featomic/)\nfor each structure in the our dataset using the hyperparameters\ndefined below. The features are constructed starting from a\ndescription of a structure in terms of atom density, for which we\ndefine the width of the overlaying Gaussians in the\n``density`` hyperparameter. The features are\ndiscretized on a basis of spherical harmonics,\nwhich consist of a radial and an angular part,\nwhich can be specified in the ``basis`` hyperparameter. The ``cutoff``\nhyperparameter controls the extent of the atomic environment.\nFor the simple example we demonstrate here,\nthe atom and pairwise features have very similar hyperparameters,\nexcept for the cutoff radius, which is larger for pairwise\nfeatures to include many pairs that describe the individual\natom-atom interaction.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "hypers = {\n    \"cutoff\": {\"radius\": 2.5, \"smoothing\": {\"type\": \"ShiftedCosine\", \"width\": 0.1}},\n    \"density\": {\"type\": \"Gaussian\", \"width\": 0.3},\n    \"basis\": {\n        \"type\": \"TensorProduct\",\n        \"max_angular\": 4,\n        \"radial\": {\"type\": \"Gto\", \"max_radial\": 5},\n    },\n}\n\nhypers_pair = {\n    \"cutoff\": {\"radius\": 3.0, \"smoothing\": {\"type\": \"ShiftedCosine\", \"width\": 0.1}},\n    \"density\": {\"type\": \"Gaussian\", \"width\": 0.3},\n    \"basis\": {\n        \"type\": \"TensorProduct\",\n        \"max_angular\": 4,\n        \"radial\": {\"type\": \"Gto\", \"max_radial\": 5},\n    },\n}\n\nfeatures = compute_features_for_target(\n    ml_data, device=DEVICE, hypers=hypers, hypers_pair=hypers_pair\n)\nml_data._set_features(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare Dataloaders\nTo efficiently feed data into the model during training,\nwe use data loaders. These handle batching and shuffling\nto optimize training performance. ``get_dataloader``\ncreates data loaders for training, validation and testing.\nThe ``model_return=\"blocks\"`` argument determines that the\nmodel targets the different blocks that the Hamiltonian is\ndecomposed into and the ``batch_size`` argument defines the\nnumber of samples per batch for the batch-wise training.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_dl, val_dl, test_dl = get_dataloader(\n    ml_data, model_return=\"blocks\", batch_size=BATCH_SIZE\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare Training\n\nNext, we set up our linear model that predicts the Hamiltonian\nmatrices, using ``LinerTargetModel``. To improve the model\nconvergence, we first start with a symmetry-adapted ridge regression\ntargeting the Hamiltonian matrices from the STO-3G basis QM\ncalculation using the ``fit_ridge_analytical`` function.\nThis provides us a more reliable set of weights to initialise the\nfine-tuning rather than starting from any random guess,\neffectively saving us training time by starting the training process\ncloser to the desired minumum.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = LinearTargetModel(\n    dataset=ml_data, nlayers=1, nhidden=16, bias=False, device=DEVICE\n)\n\npred_ridges, ridges = model.fit_ridge_analytical(\n    alpha=np.logspace(-8, 3, 12),\n    cv=3,\n    set_bias=False,\n)\n\npred_fock = model.forward(\n    ml_data.feat_train,\n    return_type=\"tensor\",\n    batch_indices=ml_data.train_idx,\n    ridge_fit=True,\n    add_noise=NOISE,\n)\n\nwith io.capture_output() as captured:\n    all_mfs, fockvars = instantiate_mf(\n        ml_data,\n        fock_predictions=None,\n        batch_indices=list(range(len(ml_data.structures))),\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training: Indirect learning of the MO energies\n\nNow rather than explicitly targeting the Hamiltonian matrix,\nwe instead treat it as an intermediate layer in our framework, where\nthe model predicts the Hamiltonian, the model weights, however,\nare subsequently fine-tuned by backpropagating\na loss on a derived molecular property of the Hamiltonian such as\nthe MO energies but from the larger def-TZVP basis instead of the\nSTO-3G basis.\n\nBefore fine-tuning the model we preconditioned\nby a ridge regression fit of our data,\nwe set up the loss function to target the MO energies,\nas well as the optimizer\nand the learning rate scheduler for our model. We use a customized\nmean squared error (MSE) loss function that guides the learning and\n``Adam`` optimizer that performs a stochastic gradient descent\nthat minimizes the error. The scheduler reduces the learning\nrate by the given factor if the validation loss plateaus.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss_fn = mlmetrics.mse_per_atom\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    factor=0.5,\n    patience=3,  # 20,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use a ``Trainer`` class to encapsulate all training logic.\nIt manages the training and validation loops.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(model, optimizer, scheduler, DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define necessary arguments for the training and validation process.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fit_args = {\n    \"ml_data\": ml_data,\n    \"all_mfs\": all_mfs,\n    \"loss_fn\": loss_fn,\n    \"weight_eva\": 1,\n    \"weight_dipole\": 0,\n    \"weight_polar\": 0,\n    \"ORTHOGONAL\": ORTHOGONAL,\n    \"upscale\": True,\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With these steps complete, we can now train the model.\nIt begins training and validation\nusing the structured molecular data, features, and defined parameters.\nThe ``fit`` function returns the training and validation losses for\neach epoch, which we can then use to plot the `loss versus epoch` curve.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "history = trainer.fit(\n    train_dl,\n    val_dl,\n    NUM_EPOCHS,\n    EARLY_STOP_CRITERION,\n    FOLDER_NAME,\n    VERBOSE,\n    DUMP_HIST,\n    **fit_args,\n)\n\nnp.save(f\"{FOLDER_NAME}/model_output/loss_stats.npy\", history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot Loss\n\nWith the help of ``plot_losses`` function we can conveniently\nplot the training and validation losses.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_losses(history, save=True, savename=f\"{FOLDER_NAME}/loss_vs_epoch.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parity Plot\nWe then evaluate the prediction of the fine-tuned model on\nthe MO energies by comparing it the with the MO energies\nfrom the reference def2-TZVP calculation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "f_pred = model.forward(\n    ml_data.feat_test,\n    return_type=\"tensor\",\n    batch_indices=ml_data.test_idx,\n)\n\ntest_eva_pred = compute_eigvals(\n    ml_data, f_pred, range(len(ml_data.test_idx)), orthogonal=ORTHOGONAL\n)\n\n# The parity plot\n# below shows the performance of our model on the test\n# dataset. ML predictions are shown with blue points and\n# the corresponding MO energies from the STO-3G basis are\n# are shown in grey.\n\n\ndef plot_parity_properties(\n    molecule_data,\n    ml_data,\n    Hartree,\n    properties=\"eva\",  # can be single string or list of strings\n    predictions_dict=None,  # dictionary with keys: \"eva\", \"dip\", \"pol\"\n):\n    # Labels, units, and axis ranges\n    prop_info_map = {\n        \"eva\": {\"label\": \"MO Energies\", \"unit\": \"eV\", \"range\": [-35, 30]},\n        \"dip\": {\"label\": \"dipoles\", \"unit\": \"a.u.\", \"range\": [-2.5, 3]},\n        \"pol\": {\"label\": \"polarisability\", \"unit\": \"a.u.\", \"range\": [-25, 150]},\n    }\n\n    if type(properties) is list:\n        n = len(properties)\n    elif type(properties) is str:\n        properties = [properties]\n        n = 1\n    else:\n        print(\"Properties input should be string or list\")\n\n    fig, axes = plt.subplots(1, n, figsize=(6 * n, 6))\n    if type(axes) is not list:\n        axes = [axes]  # Make iterable\n\n    for i, propert in enumerate(properties):\n        ax = axes[i]\n        label = prop_info_map[propert][\"label\"]\n        unit = prop_info_map[propert][\"unit\"]\n        min_val, max_val = prop_info_map[propert][\"range\"]\n\n        ax.set_axisbelow(True)\n        ax.grid(True, which=\"both\", linestyle=\"-\", linewidth=1, alpha=0.7)\n        ax.plot(\n            [min_val, max_val],\n            [min_val, max_val],\n            linestyle=\"--\",\n            color=\"black\",\n            linewidth=1.5,\n        )\n\n        # Reference vs low-basis\n        target_vals = []\n        lb_vals = []\n\n        for idx in ml_data.test_idx:\n            target = molecule_data.target[propert][idx]\n            lb = molecule_data.lb_target[propert][idx]\n            if propert == \"eva\":\n                lb = lb[: target.shape[0]]\n            target_vals.append(target)\n            lb_vals.append(lb)\n\n        target_tensor = torch.cat(target_vals)\n        lb_tensor = torch.cat(lb_vals)\n        x_vals = lb_tensor.detach().numpy().flatten()\n        y_vals = target_tensor.detach().numpy().flatten()\n\n        if propert == \"eva\":\n            x_vals *= Hartree\n            y_vals *= Hartree\n\n        ax.scatter(\n            x_vals,\n            y_vals,\n            color=\"gray\",\n            alpha=0.7,\n            s=200,\n            marker=\"o\",\n            edgecolor=\"black\",\n            label=\"STO-3G\",\n        )\n\n        # Use saved predictions\n        pred_array = predictions_dict[propert]\n        if propert == \"eva\":\n            pred_array = np.concatenate(\n                [p[: t.shape[0]] for p, t in zip(pred_array, target_vals)]\n            )\n            y_model = pred_array * Hartree\n            x_model = x_vals\n        elif propert == \"dip\":\n            y_model = pred_array\n            x_model = x_vals\n        elif propert == \"pol\":\n            y_model = pred_array\n            x_model = x_vals\n        else:\n            print(f\"Unknown property: {propert}\")\n            continue\n\n        ax.scatter(\n            x_model,\n            y_model,\n            color=\"royalblue\",\n            alpha=0.7,\n            s=200,\n            marker=\"o\",\n            edgecolor=\"black\",\n            label=r\"indirect $\\mathbf{H}$ model\",\n        )\n\n        ax.set_xlabel(f\"Target {label} ({unit})\", fontsize=16, fontweight=\"bold\")\n        ax.set_ylabel(f\"Predicted {label} ({unit})\", fontsize=16, fontweight=\"bold\")\n        ax.set_xlim(min_val, max_val)\n        ax.set_ylim(min_val, max_val)\n        ax.legend(fontsize=14)\n\n    plt.tight_layout()\n    # plt.savefig(f\"{FOLDER_NAME}/parity_combined.png\", bbox_inches=\"tight\", dpi=300)\n    plt.show()\n\n\npredictions_dict = {\n    \"eva\": [p.detach().numpy() for p in test_eva_pred],\n}\nplot_parity_properties(\n    molecule_data,\n    ml_data,\n    Hartree,\n    properties=[\"eva\"],\n    predictions_dict=predictions_dict,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can observe from the parity plot that even with a\nminimal basis parametrisation, the model is able to reproduce\nthe large basis MO energies with good accuracy. Thus, using\nan indirect model, makes it possible to promote the model\naccuracy to a higher level of theory, at no additional cost.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Example of Targeting Multiple Properties\n\nIn principle we can also target multiple properties for\nthe indirect training. While MO energies can be computed\nby simply diagonalizing the Hamiltonian matrix, some\nproperties like the dipole moment require\nthe position operator integral and its derivative if\nwe want to backpropagate the loss. We therefore interface\nour ML model with an electronic structure code that supports\nautomatic differentiation, [PySCFAD](https://github.com/fishjojo/pyscfad),\nan end-to-end auto-differentiable version of PySCF. By\ndoing so we delegate the computation of properties to PySCFAD,\nwhich provides automatic differentiation of observables with\nrespect to the intermediate Hamiltonian. In particular, we will now\nindirectly target the dipole moment and polarisability along\nwith the MO energies from a large basis reference calculation\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get Data and Prepare Data Set\nIn our last example even though we show an indirect ML model\nthat was trained on a homogenous dataset of different\nconfigurations of ethane, we can also easily extend the\nframework to use  a much diverse dataset such as the\n[QM7 dataset](http://quantum-machine.org/datasets/).\nFor our next example we select a subset of 150 structures from\nthis dataset that consists of only C, H, N and O atoms.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set parameters for training\nSet the parameters for the training, including the dataset set size\nand split, the batch size, learning rate and weights for the individual\ncomponents of eigenvalues, dipole and polarisability.\nWe additionally define a folder name, in which the results are saved.\nOptionally, noise can be added to the ridge regression fit.\n\nHere, we now need to provide different weights\nfor the different targets (eigenvalues\n$\\epsilon$, the dipole moment\n$\\mu$, and polarisability\n$\\alpha$), which we will use when computing the loss $\\mathcal{L}$.\n\n\\begin{align}\\mathcal{L}_{\\epsilon,\\mu,\\alpha} = & \\;\n   \\frac{\\omega_{\\epsilon}}{N} \\sum_{n=1}^{N} \\frac{1}{O_n} \\\n   \\sum_{o=1}^{O_n} \\left( \\epsilon_{no} - \\tilde{\\epsilon}_{no} \\right)^2 \\\n    + \\frac{\\omega_{\\mu}}{N} \\sum_{n=1}^{N} \\frac{1}{N_A^2} \\\n   \\sum_{m=1}^{N_A} \\left( \\mu_{nm} - \\tilde{\\mu}_{nm} \\right)^2 \\\\\n   & + \\frac{\\omega_{\\alpha}}{N} \\sum_{n=1}^{N} \\frac{1}{N_A^2} \\\n   \\sum_{m=1}^{N_A} \\left( \\alpha_{nm} - \\tilde{\\alpha}_{nm} \\right)^2\\end{align}\n\nwhere\n$N$ is the number of training points,\n$O_n$ is the number of MO orbitals in the nth molecule,\n$N_A$ is the number of atoms $i$.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The weights\n$\\omega$ in the loss are based on the magnitude of errors\nfor different properties, where at the end we want each of them to\ncontribute equally to the loss.\nThe following values worked well for the QM7 example, but of\ncourse depending on the system that one investigates another set\nof weights might work better.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "NUM_FRAMES = 150\nLR = 1e-3\nW_EVA = 1e4\nW_DIP = 1e3\nW_POL = 1e2\n\nFOLDER_NAME = \"output/qm7\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Datasets\n\nWe use the dataloader of the\n[mlelec package (qm7 branch)](https://github.com/curiosity54/mlelec/tree/qm7),\nand load the QM7\ndataset we downloaded above from zenodo\nfor the defined number of frames.\nFirst, we load all relavant data (geometric structures,\nauxiliary matrices -overlap and orbitals-, and\ntargets -fock, dipole moment, and polarisablity-) into a molecule dataset.\nWe do this for the minimal (STO-3G), as well as a larger basis (lb, def2-TZVP).\nThe larger basis has additional basis functions on the valence electrons.\nThe dataset, we can then load into our dataloader ```ml_data```, together with some\nsettings on how we want to sample data from the dataloader.\nFinally, we define the desired dataset split for training, validation,\nand testing from the parameters defined in example 1.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "molecule_data = MoleculeDataset(\n    mol_name=\"qm7\",\n    use_precomputed=False,\n    path=\"hamiltonian-qm7-data/qm7\",\n    aux_path=\"hamiltonian-qm7-data/qm7/sto-3g\",\n    frame_slice=slice(0, NUM_FRAMES),\n    device=DEVICE,\n    aux=[\"overlap\", \"orbitals\"],\n    lb_aux=[\"overlap\", \"orbitals\"],\n    target=[\"fock\", \"eva\", \"dip\", \"pol\"],\n    lb_target=[\"fock\", \"eva\", \"dip\", \"pol\"],\n)\n\nml_data = MLDataset(\n    molecule_data=molecule_data,\n    device=DEVICE,\n    model_strategy=\"coupled\",\n    shuffle=True,\n    shuffle_seed=SHUFFLE_SEED,\n    orthogonal=ORTHOGONAL,\n)\n\nml_data._split_indices(\n    train_frac=TRAIN_FRAC, val_frac=VALIDATION_FRAC, test_frac=TEST_FRAC\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compute Features\nThe feature hyperparameters here are similar to the\nones we used for our previous training, except the `cutoff`.\nFor a dataset like QM7 which contains molecules with over\n15 atoms, we need a slightly larger cutoff than the ones\nwe used in case of ethane. We choose a cutoff of 3 and 5\nfor the atom-centred and pair-centred features, respectively.\nNote that the computation of the features takes some time and\nrequires a large amount of memory.\n\nThis is why in the following\nof this example, we are not executing the commands related to\nthe features (that is feature computation, training, and evaluation),\nbut provide all python commands necessary if one\nwould want to do this.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "hypers = {\n    \"cutoff\": {\"radius\": 3, \"smoothing\": {\"type\": \"ShiftedCosine\", \"width\": 0.1}},\n    \"density\": {\"type\": \"Gaussian\", \"width\": 0.3},\n    \"basis\": {\n        \"type\": \"TensorProduct\",\n        \"max_angular\": 4,\n        \"radial\": {\"type\": \"Gto\", \"max_radial\": 5},\n    },\n}\n\nhypers_pair = {\n    \"cutoff\": {\"radius\": 5, \"smoothing\": {\"type\": \"ShiftedCosine\", \"width\": 0.1}},\n    \"density\": {\"type\": \"Gaussian\", \"width\": 0.3},\n    \"basis\": {\n        \"type\": \"TensorProduct\",\n        \"max_angular\": 4,\n        \"radial\": {\"type\": \"Gto\", \"max_radial\": 5},\n    },\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\nfeatures = compute_features_for_target(\n    ml_data, device=DEVICE, hypers=hypers, hypers_pair=hypers_pair\n)\nml_data._set_features(features)\n\ntrain_dl, val_dl, test_dl = get_dataloader(\n    ml_data, model_return=\"blocks\", batch_size=BATCH_SIZE\n)\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Depending on the diversity of the structures in the datasets, it may\nhappen that some blocks are empty, because certain structural\nfeatures are only present in certain structures (e.g. if we\nwould have some organic molecules with oxygen and some without).\nAs this is the case for the QM7 example,\nwe drop these blocks, so that the dataloader does not\ntry to load them during training.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\nml_data.target_train, ml_data.target_val, ml_data.target_test = drop_zero_blocks(\nml_data.target_train, ml_data.target_val, ml_data.target_test)\n\nml_data.feat_train, ml_data.feat_val, ml_data.feat_test = drop_zero_blocks(\nml_data.feat_train, ml_data.feat_val, ml_data.feat_test)\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare training\n\nHere again we first fit a ridge regression model to the data.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\nmodel = LinearTargetModel(\n    dataset=ml_data, nlayers=1, nhidden=16, bias=False, device=DEVICE\n)\n\npred_ridges, ridges = model.fit_ridge_analytical(\n    alpha=np.logspace(-8, 3, 12),\n    cv=3,\n    set_bias=False,\n)\n\npred_fock = model.forward(\n    ml_data.feat_train,\n    return_type=\"tensor\",\n    batch_indices=ml_data.train_idx,\n    ridge_fit=True,\n    add_noise=NOISE,\n)\n\nwith io.capture_output() as captured:\n    all_mfs, fockvars = instantiate_mf(\n        ml_data,\n        fock_predictions=None,\n        batch_indices=list(range(len(ml_data.structures))),\n    )\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training parameters and training\n\nFor finetuning on multiple targets we again\ndefine a loss function, optimizer and scheduler.\nWe also define the necessary arguments for training\nand validation.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\nloss_fn = mlmetrics.mse_per_atom\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    factor=0.5,\n    patience=10,\n    )\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n# Initialize trainer\ntrainer = Trainer(model, optimizer, scheduler, DEVICE)\n\n# Define necessary arguments for the training and validation process\nfit_args = {\n    \"ml_data\": ml_data,\n    \"all_mfs\": all_mfs,\n    \"loss_fn\": loss_fn,\n    \"weight_eva\": W_EVA,\n    \"weight_dipole\": W_DIP,\n    \"weight_polar\": W_POL,\n    \"ORTHOGONAL\": ORTHOGONAL,\n    \"upscale\": True,\n}\n\n\n# Train and validate\nhistory = trainer.fit(\n    train_dl,\n    val_dl,\n    200,\n    EARLY_STOP_CRITERION,\n    FOLDER_NAME,\n    VERBOSE,\n    DUMP_HIST,\n    **fit_args,\n)\n\n# Save the loss history\nnp.save(f\"{FOLDER_NAME}/model_output/loss_stats.npy\", history)\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This training can take some time to converge fully.\nThus, for this example,\nwe load a previously trained model to continue.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating the trained model\n\nA previously trained model for the QM7 dataset we are using here\nis also part of the data\ndownloaded from Zenodo.\n\n```python\nmodel.load_state_dict(torch.load(\"hamiltonian-qm7-data/qm7/output/qm7_eva_dip_pol/best_model.pt\"))\n```\nWe can then compute different properties from the trained model.\n\n```python\nbatch_indices = ml_data.test_idx\ntest_fock_predictions = model.forward(\n    ml_data.feat_test,\n    return_type=\"tensor\",\n    batch_indices=ml_data.test_idx,\n)\ntest_dip_pred, test_polar_pred, test_eva_pred = (\n    compute_batch_polarisability(\n         ml_data,\n        test_fock_predictions,\n        batch_indices=batch_indices,\n        mfs=all_mfs,\n        orthogonal=ORTHOGONAL,\n    )\n)\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot loss\nAs we have not performed training, we cannot plot the\ntraining and validation losses\nfrom the history, as we did for example 1. In principle,\nif training would have been performed,\nthe python command would be the same:\n\n```python\nplot_losses(history, save=True, savename=f\"{FOLDER_NAME}/loss_vs_epoch.pdf\")\n```\nFor reference, we provide the loss plot we obtained during training of the\nsaved model that we load above.\n\n.. figure:: loss_vs_epoch.png\n   :alt: loss versus epoch curves for training and\n         validation losses.  The MSE on MO energies, dipole moments\n         and polarisability are shown separately.\n\nThe plot shows the Loss versus Epoch curves\nfor training and validation losses. The MSE on\nMO energies, dipole moments and polarisability\nare shown separately.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parity plot\n\nWe finally want to investigate the performance of the finetuned model\nthat target the MO energies, dipole moments and polarisibaility\nof from a def2-TZVP calculation on the QM7 test dataset.\nThis can be done with the following python command:\n\n```python\npredictions_dict = {\n    \"eva\": [p.detach().numpy() for p in test_eva_pred],\n    \"dip\": test_dip_pred.detach().numpy(),\n    \"pol\": test_polar_pred.detach().numpy(),\n}\n\nplot_parity_properties(\n    molecule_data,\n    ml_data,\n    Hartree,\n    properties=[\"eva\", \"dip\", \"pol\"],\n    predictions_dict=predictions_dict\n)\n```\nThis command generates a parity plot for the desired properties.\nAs we did not compute\nthe features of the QM7 dataset for time and memory reasons,\nwe do here not execute the python command above but provide directly the\nparity plot as Figure.\n\n.. figure:: parity_plots_combined.png\n   :alt: Performance of the indirect model on the QM7 test\n          dataset, for the (a) MO energy (b) dipole moments and (c)\n          polarizability. Targets are computed with the def2-TZVP\n          basis. Gray circles correspond to the values obtained from\n          STO-3G calculations, while the blue ones correspond to val-\n          ues computed from minimal-basis Hamiltonians predicted by\n          the ML model.\n\nGray circles correspond to the values obtained from\nSTO-3G calculations, while the blue ones correspond to values\ncomputed from the reduced-basis Hamiltonians predicted by\nthe ML model.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}