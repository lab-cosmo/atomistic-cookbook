{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Equivariant model for tensorial properties based on scalar features\n\n:Authors: Paolo Pegolo [@ppegolo](https://github.com/ppegolo)\n\nIn this example, we demonstrate how to train a [metatensor atomistic model](https://docs.metatensor.org/latest/atomistic) on dipole moments and polarizabilities\nof small molecular systems, using a model that combines scalar descriptors\nwith equivariant tensorial components that depend in a simple way from\nthe molecular geometry. You may also want to read this\n[recipe for a linear polarizability model](http://localhost:8000/examples/polarizability/polarizability.html),\nwhich provides an alternative approach for tensorial learning.\nThe model is trained with\n[metatrain](https://metatensor.github.io/metatrain/latest/index.html)\nand can then be used in an ASE calculator.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_path = '../../examples/learn-tensors-with-mcov/architecture.png' # noqa: E501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Core packages\nimport subprocess\nfrom glob import glob\n\nimport ase.io\n\n# Simulation and visualization tools\nimport chemiscope\nimport matplotlib.pyplot as plt\nimport metatensor as mts\nimport numpy as np\n\n# Model wrapping and execution tools\nfrom featomic.clebsch_gordan import cartesian_to_spherical\nfrom metatensor import Labels, TensorBlock, TensorMap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the training data\nWe load a simple dataset of small molecules from the [QM7-X dataset](https://doi.org/10.1038/s41597-021-00812-2)_ spanning the CHNO composition space.\nWe extract their dipole moments and polarizability tensors stored in extended XYZ\nformat.\nWe also visualize dipoles as arrows and polarizabilities as ellipsoids with\n[chemiscope](https://chemiscope.org/)_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "molecules = ase.io.read(\"data/qm7x_reduced_100_CHNO.xyz\", \":\")\n\narrows = chemiscope.ase_vectors_to_arrows(molecules, \"mu\", scale=5)\narrows[\"parameters\"][\"global\"][\"color\"] = \"#008000\"\n\nellipsoids = chemiscope.ase_tensors_to_ellipsoids(molecules, \"alpha\", scale=0.15)\nellipsoids[\"parameters\"][\"global\"][\"color\"] = \"#FF8800\"\ncs = chemiscope.show(\n    molecules,\n    shapes={\"mu\": arrows, \"alpha\": ellipsoids},\n    mode=\"structure\",\n    settings=chemiscope.quick_settings(\n        structure_settings={\"shape\": [\"mu\", \"alpha\"]},\n        trajectory=True,\n    ),\n)\n\ncs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare the target tensors for training\nWe first split the dataset into training, validation, and test sets.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n\nindices = np.arange(len(molecules))\nn_train = int(0.80 * len(molecules))\nn_val = int(0.10 * len(molecules))\nn_test = int(0.10 * len(molecules))\n\nnp.random.shuffle(indices)\n\ntrain_indices = indices[:n_train]\nval_indices = indices[n_train : n_train + n_val]\ntest_indices = indices[n_train + n_val : n_train + n_val + n_test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each split, we extract dipole moments and polarizability tensors from the extended\nXYZ file and create a :class:`metatensor.torch.TensorMap` containing their Cartesian\ncomponents.\nSince our machine-learning model uses spherical tensors, we convert Cartesian tensors\ninto their irreducible spherical form via Clebsch-Gordan coupling, using\n:func:`featomic.clebsch_gordan.cartesian_to_spherical`. What is happening under the\nhood is:\n\n1. We reorder tensor components from the Cartesian $x$, $y$, $z$,\n   which correspond to the spherical $m=1,-1,0$, to the standard ordering\n   $m=-1,0,1$.\n2. Dipoles moments ($\\lambda=1$) require no further operations. For\n   polarizabilties we need to couple the resulting components:\n\n   .. math::\n\n     \\alpha_{\\lambda\\mu} = \\sum_{m_1,m_2} C^{\\lambda \\mu}_{m_1 m_2} \\alpha_{m_1 m_2}\n\n   where $C^{\\lambda m}_{m_1 m_2}$ are the Clebsch-Gordan\n   coefficients.\n   Since the polarizability is a symmetric rank-2 Cartesian tensor, only the\n   $\\lambda=0,2$ components are non-zero For example, the $\\lambda=0$\n   component is proportional to the trace of the Cartesian tensor:\n\n   .. math::\n\n     \\alpha_{\\lambda=0} = -\\frac{1}{\\sqrt{3}} \\left( \\alpha_{xx} + \\alpha_{yy} +\n     \\alpha_{zz} \\right)\n\nAfter the conversion, we save the spherical tensors into ``metatensor``  sparse\nformat.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for idx, filename in zip(\n    [train_indices, val_indices, test_indices], [\"training\", \"validation\", \"test\"]\n):\n    subset = [molecules[i] for i in idx]\n    ase.io.write(\n        f\"{filename}_set.xyz\",\n        subset,\n        write_info=False,\n    )\n\n    # Create Cartesian tensormaps\n    mu = np.array([molecule.info[\"mu\"] for molecule in subset])\n    cartesian_mu = TensorMap(\n        Labels.single(),\n        [\n            TensorBlock(\n                samples=Labels(\"system\", np.arange(len(subset)).reshape(-1, 1)),\n                components=[Labels.range(\"xyz\", 3)],\n                properties=Labels.single(),\n                values=mu.reshape(len(subset), 3, 1),\n            )\n        ],\n    )\n\n    alpha = np.array([molecule.info[\"alpha\"].reshape(3, 3) for molecule in subset])\n    cartesian_alpha = TensorMap(\n        Labels.single(),\n        [\n            TensorBlock(\n                samples=Labels(\"system\", np.arange(len(subset)).reshape(-1, 1)),\n                components=[Labels.range(f\"xyz_{i}\", 3) for i in range(1, 3)],\n                properties=Labels.single(),\n                values=alpha.reshape(len(subset), 3, 3, 1),\n            )\n        ],\n    )\n\n    # Convert Cartesian to spherical tensormaps\n    spherical_mu = mts.remove_dimension(\n        cartesian_to_spherical(cartesian_mu, [\"xyz\"]), \"keys\", \"_\"\n    )\n    spherical_alpha = mts.remove_dimension(\n        cartesian_to_spherical(cartesian_alpha, [\"xyz_1\", \"xyz_2\"]), \"keys\", \"_\"\n    )\n\n    # Save the spherical tensormaps to disk, ensuring contiguous memory layout\n    mts.save(f\"{filename}_dipoles.mts\", mts.make_contiguous(spherical_mu))\n    mts.save(f\"{filename}_polarizabilities.mts\", mts.make_contiguous(spherical_alpha))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The $\\lambda$-MCoV model\n\nHere is a schematic representation of the $\\lambda$-MCoV model which, in a\nnutshell, allows us to learn a tensorial property of a system from a set of scalar\nfeatures used as linear expansion coefficients of a minimal set of basis tensors.\n\n<img src=\"file://architecture.png\" align=\"center\">\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We parametrize spherical tensors of order $\\lambda$ as linear combinations\nof a small, fixed set of **maximally coupled** basis tensors. Each basis tensor is\ncomputed from three learned **vector** features, and each coefficient is predicted by\na **scalar** function of the local atomic environment. This enforces exact\nequivariance under the action of the orthogonal group O(3), while relying only on\nefficient scalar networks.\nThe architecture is composed as follows:\n\n### 1. Local Spherical Expansion\n\nWe compute atom-centered spherical expansion coefficients of the neighbor density\naround atom $i$:\n\n\\begin{align}\\boldsymbol{\\rho}_{z,nl} = \\sum_{i \\in z} R_{n,l}(r_i) \\,\n  \\boldsymbol{Y}_l(\\hat{\\mathbf{r}}_i)\\end{align}\n\nfor orders $l=1$ (vector basis) up to $l=\\lambda$ (correction).\n\n### 2. Learned Vector Basis\n\nFrom the $l=1$ coefficients, we form three global vectors by a learnable linear\nlayer over species $z$ and radial channels $n$:\n\n\\begin{align}\\end{align}\n\n \\mathbf{q}_\\alpha = \\sum_{z,n} W_{\\alpha,zn}\\,\\boldsymbol{\\rho}_{z,n1}\n\n### 3. Maximally Coupled Tensor Basis\n\nWe build the $2\\lambda+1$ independent components by **maximally coupling** the\nthree vectors $\\mathbf{q}_1,\\mathbf{q}_2,\\mathbf{q}_3$. Maximally coupled\ntensors are defined by contracting their harmonic components to the highest total\nangular momentum. For example, maximally coupling $\\lambda$ vectors yields:\n\n\\begin{align}(\\underbrace{\\mathbf{a}_1 \\widetilde{\\otimes} \\ldots \\widetilde{\\otimes}\n  \\mathbf{a}_\\lambda}_{\\lambda\\text{ times}})_{\\lambda\\mu} := \\Big(\\big(\\cdots\n  \\big((\\mathbf{a}_1 \\widetilde{\\otimes} \\mathbf{a}_2)_2 \\widetilde{\\otimes}\n  \\mathbf{a}_3\\big)_3\\widetilde{\\otimes} \\cdots \\widetilde{\\otimes}\n  \\mathbf{a}_{\\lambda-1}\\big)_{\\lambda-1}\\widetilde{\\otimes}\n  \\mathbf{a}_\\lambda\\Big)_{\\lambda\\mu} \\phantom{:}= \\sum_{m_1\\ldots m_\\lambda}\n  \\mathcal{C}^{\\lambda\\mu}_{m_1\\ldots m_\\lambda}\\big((\\mathbf{a}_1)_{m_1} \\cdot\n  \\ldots\\cdot (\\mathbf{a}_\\lambda)_{m_\\lambda}\\big),\\end{align}\n\nwhere $\\mathcal{C}^{\\lambda\\mu}_{m_1\\ldots m_\\lambda}$ a shorthand notation for\nthe components of the tensor $\\mathcal{C}$ obtained by contracting the\nClebsch-Gordan coefficients involved in the coupling.\n\nWith this definition, vector components can be expressed as\n\n\\begin{align}\\mathbf{T}_{1}(\\{\\mathbf{r}_i\\}) = \\sum_{\\alpha = 1}^3 f_{\\alpha}\n  (\\{\\mathbf{r}_i\\})\\,\\mathbf{q}_{\\alpha},\\end{align}\n\nand $\\lambda=2$ components as\n\n\\begin{align}\\mathbf{T}_{2}(\\{\\mathbf{r}_i\\}) = \\sum_{\\alpha = 1}^2 f_\\alpha(\\{\\mathbf{r}_i\\})\n  \\mathbf{q}_\\alpha^{\\widetilde{\\otimes} 2}\\,\\,+ \\sum_{\\substack{\\alpha_1,\\alpha_2=\n  1\\\\\\alpha_1<\\alpha_2}}^{3} g_{\\alpha_1\\alpha_2}(\\{\\mathbf{r}_i\\})\n  \\big(\\mathbf{q}_{\\alpha_1}\\widetilde{\\otimes} \\mathbf{q}_{\\alpha_2}\\big)_{2}.\\end{align}\n\nMore generally, for any $\\lambda>2$ we have:\n\n\\begin{align}\\mathbf{T}_{\\lambda} (\\{\\mathbf{r}_i\\}) = \\sum_{l= 0}^\\lambda f_{l}\n  (\\{\\mathbf{r}_i\\}) \\Big(\\mathbf{q}_1^{\\widetilde{\\otimes} l}\\widetilde{\\otimes}\n  {\\mathbf{q}}_2^{\\widetilde{\\otimes}(\\lambda-l)}\\Big)_{\\lambda}+\n  \\sum_{l=0}^{\\lambda-1} g_{l} (\\{\\mathbf{r}_i\\})\n  \\Big(\\mathbf{q}^{\\widetilde{\\otimes} l}_1\\widetilde{\\otimes}\n  \\mathbf{q}_2^{\\widetilde{\\otimes} (\\lambda-l-1)}\\widetilde{\\otimes}\n  \\mathbf{q}_3\\Big)_{\\lambda\\mu}.\\end{align}\n\n### 4. $\\lambda$-Correction Term\n\nHighly symmetric environments can lead to all-zero vector spherical expansion\ncomponents, which in turn would yield all-zero tensor features.\nTo correct this, we add a term based on the order $\\lambda$ spherical expansion:\n\n\\begin{align}\\mathbf{T}_\\lambda^{\\rm corr}(\\{\\mathbf{r}_i\\}) \\;=\\;\n  \\sum_{\\beta=1}^{2\\lambda+1}\n  h_\\beta(\\{\\mathbf{r}_i\\})\\,\n  \\sum_{z,n}W^{\\rm corr}_{\\beta,zn}\\,\\boldsymbol{\\rho}_{z,n}^{l=\\lambda}\\end{align}\n\nwith learnable scalar functions $h_\\beta(\\{\\mathbf{r}_i\\})$.\n\n### 5. Scalar Network (SOAP-BPNN)\n\nWe first compute **SOAP powerspectrum** features:\n\n\\begin{align}p_{z_1z_2,n_1 n_2,l} = \\bigl(\\boldsymbol{\\rho}_{z_1,n_1 l} \\widetilde{\\otimes}\n  \\boldsymbol{\\rho}_{z_2,n_2 l}\\bigr)_0\\end{align}\n\nand then apply a small, per-species multi-layer perceptron to map these features to\nscalar coefficients $f,g,h$.\n\n### 6. Assembly and Global Output\n\nFinally, we assemble the tensor:\n\n\\begin{align}\\mathbf{T}_\\lambda(\\{\\mathbf{r}_i\\}) = \\sum_{\\beta=1}^{2\\lambda+1}\n  s_\\beta(\\{\\mathbf{r}_i\\}) \\, \\mathbf{B}^\\lambda_\\beta\n  (\\mathbf{q}_1,\\mathbf{q}_2,\\mathbf{q}_3) + \\mathbf{T}_\\lambda^{\\mathrm{corr}}\n  (\\{\\mathbf{r}_i\\}),\\end{align}\n\nwhere $\\mathbf{B}^\\lambda_\\beta$ is a shorthand for the basis tensors and\n$s_\\beta$ for the scalar coefficients. For global properties we sum over all\natoms.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training and evaluation of the model\n\nRather than implementing the $\\lambda$-MCoV model from scratch, we use a\npre-defined architecture within the ``metatrain`` package, using its command-line\ninterface.\nTo start training, we run\n\n```shell\nmtt train options.yaml\n```\nThe options file specifies the model architecture and the training parameters:\n\n.. literalinclude:: options.yaml\n  :language: yaml\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To execute ``metatrain`` from within a script, use\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "subprocess.run(\n    [\n        \"mtt\",\n        \"train\",\n        \"options.yaml\",\n    ],\n    check=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We visualize training and validation losses as functions of the epoch.\nThe training log is stored in CSV format in the ``outputs`` directory.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_log = np.genfromtxt(\n    glob(\"outputs/*/*/train.csv\")[-1],\n    delimiter=\",\",\n    names=True,\n    dtype=None,\n    encoding=\"utf-8\",\n)[1:]\n\nplt.loglog(train_log[\"Epoch\"], train_log[\"training_loss\"], label=\"Training\")\nplt.loglog(train_log[\"Epoch\"], train_log[\"validation_loss\"], label=\"Validation\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.title(\"Training and validation loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We evaluate the model on the test set using the ``metatrain`` command-line interface:\n\n```shell\nmtt eval model.pt eval.yaml -e extensions -o test_results.mts\n```\nThe evaluation YAML file contains lists the structures and corresponding reference\nquantities for the evaluation:\n\n.. literalinclude:: eval.yaml\n  :language: yaml\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can evaluate the model from within the script with\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "subprocess.run(\n    [\n        \"mtt\",\n        \"eval\",\n        \"model.pt\",\n        \"eval.yaml\",\n        \"-e\",\n        \"extensions\",\n        \"-o\",\n        \"test_results.mts\",\n    ],\n    check=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We load the test set predictions and targets from disk and prepare them for\ncomparison.\nPredictions are in ``test_results_mtt::dipole.mts`` and\n``test_results_mtt::polarizability.mts``. Targets are in ``test_dipoles.mts`` and\n``test_polarizabilities.mts``. We can load them using\nthe :func:`metatensor.load` function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "prediction_test = {\n    \"dipole\": mts.load(\"test_results_mtt::dipole.mts\"),\n    \"polarizability\": mts.load(\"test_results_mtt::polarizability.mts\"),\n}\n\ntarget_test = {\n    \"dipole\": mts.load(\"test_dipoles.mts\"),\n    \"polarizability\": mts.load(\"test_polarizabilities.mts\"),\n}\n\ntest_set_molecules = ase.io.read(\"test_set.xyz\", \":\")\nnatm = np.array([len(mol) for mol in test_set_molecules])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create parity plots comparing predicted and target values for each target quantity\nand for each $\\lambda$ component.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "color_per_lambda = {0: \"C0\", 1: \"C1\", 2: \"C2\"}\n\nfig, axes = plt.subplots(1, 2)\nfor ax, key in zip(axes, prediction_test):\n\n    ax.set_aspect(\"equal\")\n\n    pred = prediction_test[key]\n    target = target_test[key]\n\n    for k in target.keys:\n        assert k in pred.keys\n        o3_lambda = int(k[\"o3_lambda\"])\n        label = rf\"$\\lambda={o3_lambda}$\"\n        x = target[k].values[..., 0] / natm[:, np.newaxis]\n        y = pred[k].values[..., 0] / natm[:, np.newaxis]\n        ax.plot(\n            x.flatten(),\n            y.flatten(),\n            \".\",\n            color=color_per_lambda[o3_lambda],\n            label=label,\n        )\n\n    xmin, xmax = ax.get_xlim()\n    ax.plot([xmin, xmax], [xmin, xmax], \"k--\", lw=1)\n    ax.set_xlim(xmin, xmax)\n    ax.set_ylim(xmin, xmax)\n\n    ax.set_xlabel(\"Target (a.u./atom)\")\n    ax.set_ylabel(\"Prediction (a.u./atom)\")\n\n    ax.set_title(key.capitalize())\n\n    ax.legend()\nfig.tight_layout()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}