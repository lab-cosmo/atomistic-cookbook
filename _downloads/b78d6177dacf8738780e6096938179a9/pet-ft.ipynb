{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Fine-tuning the PET-MAD universal potential\n\n:Authors: Davide Tisi [@DavideTisi](https://github.com/DavideTisi),\n          Zhiyi Wang [@0WellyWang0](https://github.com/0WellyWang0),\n          Cesare Malosso [@cesaremalosso](https://github.com/cesaremalosso),\n          Sofiia Chorna [@sofiia-chorna](https://github.com/sofiia-chorna)\n\nThis example demonstrates how to fine-tune the PET-MAD universal ML potential\non a new dataset using [metatrain](https://github.com/metatensor/metatrain).\nThis allows adapting the model to a specialized task by retraining it on a\nmore focused, domain-specific dataset.\n\n[PET-MAD](https://arxiv.org/abs/2503.14118) is a universal machine-learning forcefield\ntrained on [the MAD dataset](https://arxiv.org/abs/2506.19674) that aims to\nincorporate a very high degree of structural diversity.\nThe model itself is [the Point-Edge Transformer (PET)](https://proceedings.neurips.cc/paper_files/paper/2023/file/fb4a7e3522363907b26a86cc5be627ac-Paper-Conference.pdf),\nan unconstrained architecture that achieves symmetry compliance through data\naugmentation during training.\nYou can see an overview of its usage in this [introductory example](https://atomistic-cookbook.org/examples/pet-mad/pet-mad.html).\n\nThe goal of this recipe is to demonstrate the process, not to reach high accuracy.\nAdjust the dataset size and hyperparameters accordingly if adapting this for an actual\napplication.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_number = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport subprocess\nfrom collections import Counter\nfrom glob import glob\nfrom urllib.request import urlretrieve\n\nimport ase.io\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom metatrain.utils.io import model_from_checkpoint\nfrom sklearn.linear_model import LinearRegression\n\n\nif hasattr(__import__(\"builtins\"), \"get_ipython\"):\n    get_ipython().run_line_magic(\"matplotlib\", \"inline\")  # noqa: F821"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparing for fine-tuning\n\nWhile PET-MAD is trained as a universal model capable of handling a broad range of\natomic environments, fine-tuning it allows to adapt this general-purpose model to a\nmore specialized task. First, we need to get a checkpoint of the pre-trained PET-MAD\nmodel to start training from it. The checkpoint is stored in the\n[lab-codmo Hugging Face repository](https://huggingface.co/lab-cosmo/pet-mad) and can be fetched using wget or curl:\n\n```bash\nwget https://huggingface.co/lab-cosmo/pet-mad/resolve/v1.1.0/models/pet-mad-v1.1.0.ckpt # noqa: E501\n```\nWe'll download it directly:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "url = (\n    \"https://huggingface.co/lab-cosmo/pet-mad/resolve/v1.1.0/models/pet-mad-v1.1.0.ckpt\"\n)\ncheckpoint_path = \"pet-mad-v1.1.0.ckpt\"\n\nif not os.path.exists(checkpoint_path):\n    urlretrieve(url, checkpoint_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Applying an atomic energy correction\n\nDFT-calculated energies often contain systematic shifts due to the choice of\nfunctional, basis set, or pseudopotentials. If left uncorrected, such shifts can\nmislead the fine-tuning process.\n\nOn this example we use the sampled subset of ethanol structures from [rMD17 dataset](https://doi.org/10.48550/arXiv.2007.09593) with PBE/def2-SVP level of theory which\nis different from the MAD which uses PBEsol and a plane-waves basis set.\nWe apply a linear correction based on\natomic compositions to align our fine-tuning dataset with PET-MAD energy reference.\nFirst, we define a helper function to load reference energies from PET-MAD.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def load_reference_energies(checkpoint_path):\n    \"\"\"\n    Extract atomic reference energies from the PET-MAD checkpoint.\n\n    It returns a mapping of elements to their reference energies (eV), e.g.: {'1':\n    -1.23, '2': -5.67, ...}\n    \"\"\"\n    checkpoint = torch.load(checkpoint_path, weights_only=False)\n    pet_model = model_from_checkpoint(checkpoint, \"finetune\")\n\n    energy_values = pet_model.additive_models[0].model.weights[\"energy\"].block().values\n    atomic_numbers = checkpoint[\"model_data\"][\"dataset_info\"].atomic_types\n\n    return dict(zip(atomic_numbers, energy_values))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For demonstration, the dataset is composed only of 100 structures of ethanol. We fit a\nlinear model based on atomic compositions that we use as the energy correction.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset = ase.io.read(\"data/ethanol.xyz\", index=\":\", format=\"extxyz\")\n\n# Extract DFT energies and compositions\ndft_energies = [atoms.get_potential_energy() for atoms in dataset]\ncompositions = [Counter(atoms.get_atomic_numbers()) for atoms in dataset]\nelements = sorted({element for composition in compositions for element in composition})\n\nX = np.array([[comp.get(elem, 0) for elem in elements] for comp in compositions])\ny = np.array(dft_energies)\n\n# Fit linear model to estimate DFT per-element energy\ncorrection_model = LinearRegression()\ncorrection_model.fit(X, y)\n\ncoeffs = dict(zip(elements, correction_model.coef_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply a correction to each structure\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_compositional_energy(atoms, energy_per_atom):\n    \"\"\"Calculates total energy from atomic composition and per-atom energies\"\"\"\n    counts = Counter(atoms.get_atomic_numbers())\n    return sum(energy_per_atom[Z] * count for Z, count in counts.items())\n\n\n# Get reference energies from PET-MAD\nref_energies = load_reference_energies(checkpoint_path)\n\n# Apply correction\nfor atoms, E_dft in zip(dataset, dft_energies):\n    E_comp_dft = get_compositional_energy(atoms, coeffs)\n    E_comp_ref = get_compositional_energy(atoms, ref_energies)\n\n    corrected_energy = E_dft - E_comp_dft + E_comp_ref - correction_model.intercept_\n\n    atoms.info[\"energy-corrected\"] = corrected_energy.item()\n\n\n# Split corrected dataset and save it\nnp.random.seed(42)\nindices = np.random.permutation(len(dataset))\nn = len(dataset)\nn_val = n_test = int(0.1 * n)\nn_train = n - n_val - n_test\n\ntrain = [dataset[i] for i in indices[:n_train]]\nval = [dataset[i] for i in indices[n_train : n_train + n_val]]\ntest = [dataset[i] for i in indices[n_train + n_val :]]\n\nase.io.write(\"data/ethanol_train.xyz\", train, format=\"extxyz\")\nase.io.write(\"data/ethanol_val.xyz\", val, format=\"extxyz\")\nase.io.write(\"data/ethanol_test.xyz\", test, format=\"extxyz\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defines some helper functions\nWe also define a few helper functions to visualize the training results.\nEach training run generates a log, stored in CSV format in the outputs folder.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def parse_training_log(csv_file):\n    with open(csv_file, encoding=\"utf-8\") as f:\n        headers = f.readline().strip().split(\",\")\n\n    cleaned_names = [h.strip().replace(\" \", \"_\") for h in headers]\n\n    train_log = np.genfromtxt(\n        csv_file,\n        delimiter=\",\",\n        skip_header=2,\n        names=cleaned_names,\n    )\n\n    return train_log\n\n\ndef display_training_curves(train_log, ax=None, style=\"-\", label=\"\"):\n    \"\"\"Plots training and validation losses from the training log\"\"\"\n\n    if ax is None:\n        _, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n    else:\n        ax1, ax2 = ax\n\n    ax1.loglog(\n        train_log[\"Epoch\"],\n        train_log[\"training_energy_MAE_per_atom\"],\n        f\"r{style}\",\n        label=f\"Train. {label}\",\n    )\n    ax1.loglog(\n        train_log[\"Epoch\"],\n        train_log[\"validation_energy_MAE_per_atom\"],\n        f\"b{style}\",\n        label=f\"Valid. {label}\",\n    )\n\n    ax2.loglog(\n        train_log[\"Epoch\"],\n        train_log[\"training_forces_MAE\"],\n        f\"r{style}\",\n        label=\"Training, F\",\n    )\n    ax2.loglog(\n        train_log[\"Epoch\"],\n        train_log[\"validation_forces_MAE\"],\n        f\"b{style}\",\n        label=\"Validation, F\",\n    )\n\n    ax1.set_xlabel(\"Epoch\")\n    ax2.set_xlabel(\"Epoch\")\n    ax1.set_ylabel(\"Energy MAE (meV)\")\n    ax2.set_ylabel(\"Force MAE (meV/\u00c5)\")\n    ax1.legend()\n\n    return ax1, ax2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train a model from scratch\nWe first fit a PET model to the corrected dataset to establish a baseline.\nWe use the `metatrain` utility to train the model. The training is configured\nin a YAML file, which specifies training, validation and test set, model\narchitecture, optimizer settings, etc. You can learn more about\nthe different settings in the [metatrain documentation](https://metatensor.github.io/metatrain/).\n\n.. literalinclude:: from_scratch_options.yaml\n  :language: yaml\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To launch training, you just need to run the following command in the terminal:\n\n```bash\nmtt train <options.yaml> [-o <output.pt>]\n```\nOr from Python:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "subprocess.run(\n    [\"mtt\", \"train\", \"from_scratch_options.yaml\", \"-o\", \"from_scratch-model.pt\"],\n    check=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training logs are stored in the ``outputs/`` directory, with a subdirectory\nnamed by the date and time of the training run. The model checkpoint is saved as\n``model.ckpt`` and the exported model as ``model.pt``, unless specified otherwise\nwith the ``-o`` option.\n\nWe can load the latest training log and visualize the training curves\n- we will use them later to compare the fine-tuning results.\nIt is clear that training is not converged, and the learning rate is not\noptimal -- you can try to adjust the parameters and run for longer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "csv_file = sorted(glob(\"outputs/*/*/train.csv\"))[-1]\nfrom_scratch_log = parse_training_log(csv_file)\ndisplay_training_curves(from_scratch_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple model fine-tuning\n\nHaving prepared the dataset and fitted a baseline model \"from scratch\",\nwe proceed with the training of a fine-tuned model. To this end, we also use the\n``metatrain`` package. There are multiple strategies to apply\nfine-tuning, each described in the [documentation](https://metatensor.github.io/metatrain/latest/advanced-concepts/fine-tuning.html).\nIn this example we demostrate a basic full fine-tuning strategy, which adapts all\nmodel weights to the new dataset starting from the pre-trained PET-MAD checkpoint. The\nprocess is configured by setting appropriate settings in the YAML options file.\n\n.. literalinclude:: full_ft_options.yaml\n  :language: yaml\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "subprocess.run(\n    [\"mtt\", \"train\", \"full_ft_options.yaml\", \"-o\", \"fine_tune-model.pt\"], check=True\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comparing the model tranined from scratch (dashed lines) and the fine-tuned one\n(full lines), it is clear that fine-tuning from PET-MAD weights\nleads to much better zero-shot accuracy, and more consistent learning dynamics.\nObviously it may be possible to tweak differently, and it is not unlikely that\na large single-purpose training set and long training time might lead to better\nvalidation error than performing fine tuning.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "csv_file = sorted(glob(\"outputs/*/*/train.csv\"))[-1]\nfine_tune_log = parse_training_log(csv_file)\n\nax = display_training_curves(fine_tune_log, label=\"Fine tuning\")\ndisplay_training_curves(from_scratch_log, ax=ax, style=\"--\", label=\"From scratch\")\nax[0].set_ylim(1, 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model evaluation\nAfter the training, ``mtt train`` outputs the ``fine_tune-model.ckpt``\nand ``fine_tune-model.pt`` (exported fine-tuned model) files in both the\ncurrent directory and in ``output/YYYY-MM-DD/HH-MM-SS/``.\n\nThese can be used together with `metatrain` to evaluate the model on a\n(potentially different) dataset. The evaluation is configured in a YAML file,\nwhich specifies the dataset to use, and the metrics to compute.\n\n.. literalinclude:: model_eval.yaml\n  :language: yaml\n\nThe evaluation can be run from the command line:\n\n```bash\nmtt eval fine_tune-model.pt model_eval.yaml\n```\nOr from Python:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "subprocess.run([\"mtt\", \"eval\", \"fine_tune-model.pt\", \"model_eval.yaml\"], check=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}