{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Conservative fine-tuning for a PET model\n\n:Authors: Michele Ceriotti [@ceriottm](https://github.com/ceriottm/),\n          Sofiia Chorna [@sofiia-chorna](https://github.com/sofiia-chorna)\n\nThis example demonstrates a \"conservative fine-tuning\" (or equivalently,\n\"non-conservative pre-training\") strategy. This consists in training a model using\n(faster) direct, non-conservative force predictions, and then fine-tuning it\nwith backpropagated forces to achieve an accurate conservative model.\n\nAs discussed in [this paper](https://openreview.net/pdf?id=OEl3L8osas), while\nconservative MLIPs are generally better suited for physically accurate simulations,\nhybrid models that support direct non-conservative force predictions can accelerate\nboth training and inference.\nTraining can be accelerated through a two-stage approach:\nfirst train a model to predict non-conservative forces directly (avoiding the cost\nof backpropagation) and then fine-tune its energy head to produce conservative\nforces. This two-step strategy is usually faster and produces a model that can\nexploit both types of force predictions.\nAn example of how to use direct forces in molecular dynamics simulations\nsafely (i.e. avoiding unphysical behavior due to lack of energy conservation)\nis provided in [this example](https://atomistic-cookbook.org/examples/pet-mad-nc/pet-mad-nc.html).\n\nIf you are looking for traditional fine-tuning of a pre-trained universal model, see for\nexample\n[this recipe](https://atomistic-cookbook.org/examples/pet-finetuning/pet-ft.html).\nNote also that the models generated in this example are run for too short a time to\nproduce a useful model (further revealing the advantages of this direct-force\npre-training strategy). The data file contains also \"long\" training settings, which\ncan be used (preferably on a GPU) to train a model up to a point that reveals\nthe behavior of the method in more realistic conditions. It should also be noted that\nthis technique is particularly useful to speed up the training of large models on\nlarge (potentially universal) datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_path = '../../examples/pet-finetuning/training_strategy_comparison.png'  # noqa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import subprocess\nfrom time import time\n\nimport ase.io\nimport numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare the training set\n\nWe begin by creating a train/validation/test split of the dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset = ase.io.read(\"data/ethanol.xyz\", index=\":\", format=\"extxyz\")\n\nnp.random.seed(42)\nindices = np.random.permutation(len(dataset))\nn = len(dataset)\nn_val = n_test = int(0.1 * n)\nn_train = n - n_val - n_test\n\ntrain = [dataset[i] for i in indices[:n_train]]\nval = [dataset[i] for i in indices[n_train : n_train + n_val]]\ntest = [dataset[i] for i in indices[n_train + n_val :]]\n\nase.io.write(\"data/ethanol_train.xyz\", train, format=\"extxyz\")\nase.io.write(\"data/ethanol_val.xyz\", val, format=\"extxyz\")\nase.io.write(\"data/ethanol_test.xyz\", test, format=\"extxyz\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Non-conservative force training\n\n`metatrain` provides a convenient interface to train a PET model with\nnon-conservative forces. You can see the [metatrain documentation](https://metatensor.github.io/metatrain) for general examples of how\nto use this tool.\nThe key step to add non-conservative forces is to add a\n``non_conservative_forces`` section to the ``targets`` specifications of\nthe YAML file describing the training exercise\n\n.. literalinclude:: nc_train_options.yaml\n  :language: yaml\n\nAdding a ``non_conservative_forces`` target with these specifications automatically\nadds a vectorial output to the atomic heads of the model, but does not\ndisable the energy head, which is still used to compute the energy,\nand could in principle be used to compute conservative forces.\nTo profit from the speed up of direct force evaluation, we specify\n``forces: off`` in the ``energy`` target, turning off the backpropagation of forces.\n\nTraining can be run from the command line using the `mtt` command:\n\n```bash\nmtt train nc_train_options.yaml -o nc_model.pt\n```\nor from Python:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "time_nc = -time()\nsubprocess.run(\n    [\"mtt\", \"train\", \"nc_train_options.yaml\", \"-o\", \"nc_model.pt\"], check=True\n)\ntime_nc += time()\nprint(f\"Training time (non-cons.): {time_nc:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At evaluation time, we can compute both conservative and non-conservative forces.\nNote that the training run is too short to produce a decent model. If you have\na GPU and a few more minutes, you can run one of the ``long_*`` option files,\nthat provide a more realistic training setup. You can evaluate from the command line\n\n```bash\nmtt eval nc_model.pt nc_model_eval.yaml\n```\nOr from Python:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "subprocess.run(\"mtt eval nc_model.pt nc_model_eval.yaml\".split(), check=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result of a non-conservative force learning run (600 epochs) is\npresented in the parity plot below.\nThe plot shows that the model's conservative force predictions (that are\nnot trained, on the left) have larger errors than those obtained from the trained\ndirect predictions (right). The non-conservative forces align closely with the\ntargets but lack the physical constraint of being the derivatives of a potential\nenergy, often leading to unphysical behavior when used in simulations.\n\n<img src=\"file://nc_learning_res.png\" align=\"center\" width=\"700px\">\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine-tuning on conservative forces\n\nEven though the error on energy derivatives is pretty large, the model has learned\na reasonable representation of the energy of the systems, and we can use it as a\nstarting point to fine-tune the model to improve the accuracy on conservative forces.\nWe enable ``forces: on`` to compute them via backward propagation of gradients.\nWe also keep training the non-conservative forces (with a small loss weight) so that\nwe can still use the model for fast direct force inference, with minimal overhead\nagainst forward energy evaluation. Expectedly, the training will be slower.\n\n```yaml\ntraining_set:\n  systems:\n    read_from: data/ethanol_train.xyz\n    length_unit: angstrom\n  targets:\n    energy:\n      unit: eV\n      forces: on\n    non_conservative_forces:\n      key: forces\n      type:\n        cartesian:\n          rank: 1\n      per_atom: true\n```\nRun training, restarting from the previous checkpoint:\n\n```bash\nmtt train c_ft_options.yaml -o c_ft_model.pt --restart=nc_model.ckpt\n```\nOr in Python:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "time_c_ft = -time()\nsubprocess.run(\n    \"mtt train c_ft_options.yaml -o c_ft_model.pt --restart=nc_model.ckpt\".split(\" \"),\n    check=True,\n)\ntime_c_ft += time()\nprint(f\"Training time (conservative fine-tuning): {time_c_ft:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's evaluate the forces again.\n\n```bash\nmtt eval nc_model.pt nc_model_eval.yaml\n```\nOr from Python:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "subprocess.run(\"mtt eval nc_model.pt nc_model_eval.yaml\".split(), check=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Converged results\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>To reproduce the results shown below, run extended training simulations using the\n   provided YAML configuration files. These runs are longer, and too slow to be\n   executed on a CPU in the automated testing framework of the cookbook, so we provide\n   static images to comment on the outcomes.</p></div>\n\nWe extend the training time with more epochs to obtain improved predictive\nperformance. Below are the full YAML parameter sets used for the long non-conservative\nand conservative runs:\n\n.. raw:: html\n\n   <details>\n   <summary>Non-conservative training (NC) YAML</summary>\n\n.. literalinclude:: long_nc_options.yaml\n   :language: yaml\n\n.. raw:: html\n\n   </details>\n\n.. raw:: html\n\n   <details>\n   <summary>Conservative fine-tuning (C) YAML</summary>\n\n.. literalinclude:: long_c_ft_options.yaml\n   :language: yaml\n\n.. raw:: html\n\n   </details>\n\nAfter conservative fine-tuning for 2400 epochs, the updated parity plots show improved\nforce predictions (left) with conservative forces. The grayscale points in the\nbackground correspond to the predicted forces from the non-conservative step.\n\n<img src=\"file://c_ft_res.png\" align=\"center\" width=\"700px\">\n\nThe figure below compares the validation force MAE as a function of GPU hours for\ndirect training of the conservative PET model (\"C-only\") and a two-step approach:\ninitial training of a non-conservative model followed by conservative training\ncontinuation. For the given GPU hours frame, the two-step approach yields lower\nvalidation error (or achieve the same accuracy in a shorter time).\n\n<img src=\"file://training_strategy_comparison.png\" align=\"center\" width=\"700px\">\n\nTraining on a larger dataset and performing some hyperparameter optimization could\nfurther improve performance.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}