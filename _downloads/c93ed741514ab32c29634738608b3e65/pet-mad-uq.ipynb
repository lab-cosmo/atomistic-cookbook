{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Uncertainty Quantification with PET-MAD\n\n:Authors: Johannes Spies [@johannes-spies](https://github.com/johannes-spies)\n\nThis recipe demonstrates three ways of computing errors on the outputs of\nML potential-driven simulations, using as an example the PET-MAD model and its\nbuilt-in uncertainty quantification (UQ) capabilities.\n\nIn particular, it demonstrates:\n\n1. Estimating uncertainties for single-point calculations on a\n   full validation dataset.\n2. Computing energies in simple functions of energy predictions,\n   namely the value of vacancy formation energies\n3. Propagating errors from energy predictions to thermodynamic averages\n   computed over a constant-temperature MD simulation.\n\n\nFor more information on PET-MAD, have a look at\n[Mazitov et al., 2025.](https://arxiv.org/abs/2503.14118)\nThe LLPR uncertainties are introduced in [Bigi et al., 2024.](https://arxiv.org/abs/2403.02251) For more\ninformation on dataset calibration and error propagation, see\n[Imabalzano et al., 2021.](https://arxiv.org/abs/2011.08828)\n\n## Optional: Adding UQ to a Model\n\nModels compatible with [metatomic](https://metatensor.github.io/metatomic/) can be\nequipped with UQ capabilities through the\n`LLPRUncertaintyModel` wrapper included with\n[metatrain](https://metatensor.github.io/metatrain/). For running this recipe, you can\nuse a prebuilt model (the example itself downloads a model from Hugging Face).\nFor adding UQ support to an existing model, have a look at\nthe following scaffold. For more information on loading a dataset with the\ninfrastructure, have a look at\n[this section](https://metatensor.github.io/metatrain/latest/dev-docs/utils/data)\nof the documentation. The pseudocode below also shows how to create an ensemble model\nfrom the last-layer parameters of a model.\n\n```python\nfrom metatrain.utils.llpr import LLPRUncertaintyModel\nfrom metatomic.torch import AtomisticModel, ModelMetadata\n\n# You need to provide a model and datasets (wrapped in PyTorch dataloaders).\nmodel = ...\ndataloaders = {\"train\": ..., \"val\": ...}\n\n# Wrap the model in a module capable of estimating uncertainties, estimate the\n# inverse covariance on the training set, and calibrate the model on the validation\n# set.\nllpr_model = LLPRUncertaintyModel(model)\nllpr_model.compute_covariance(dataloaders[\"train\"])\nllpr_model.compute_inverse_covariance(regularizer=1e-4)\nllpr_model.calibrate(dataloaders[\"val\"])\n\n# In the next step, we show how to enable ensembles in PET-MAD. For that, it is\n# necessary to extract the last-layer parameters of the model. The ensemble\n# generation expects the parameters in a flat-vector format.\nlast_layer_parameters = ...\n\n# Generate an ensemble with 128 members to compare ensemble uncertainties to LLPR\n# scores.\nllpr_model.generate_ensemble({\"energy\": last_layer_parameters}, 128)\n\n# Save the model to disk using metatomic.\nexported_model = AtomisticModel(\n    llpr_model.eval(),\n    ModelMetadata(),\n    llpr_model.capabilities,\n)\nexported_model.save(\"models/model-with-llpr.pt\")\n```\n## Getting Started\n\nAt the bottom of the page, you'll find a ZIP file containing the whole example. Note\nthat it comes with an `environment.yml` file specifying all dependencies required\nto execute the script.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport subprocess\nfrom urllib.request import urlretrieve\n\nimport ase.geometry.rdf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom ase import Atoms\nfrom ase.filters import FrechetCellFilter\nfrom ase.io.cif import read_cif\nfrom ase.optimize.bfgs import BFGS\nfrom ipi.utils.scripting import InteractiveSimulation\nfrom metatomic.torch import ModelOutput\nfrom metatomic.torch.ase_calculator import MetatomicCalculator\nfrom metatrain.utils.data import Dataset, read_systems, read_targets\nfrom metatrain.utils.data.system_to_ase import system_to_ase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Loading\nAll examples require a PET-MAD model with ensemble and LLPR prediction. The\nfollowing\ncode loads a pre-trained model using the ASE-compatible calculator wrapper. Using the\ncalculator instead of calling the model directly conveniently hides computing\nneighbor lists in the calculator.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"models/pet-mad-latest-llpr.pt\"):\n    os.makedirs(\"models\", exist_ok=True)\n    urlretrieve(\n        \"https://huggingface.co/jospies/pet-mad-llpr/resolve/main/\"\n        \"pet-mad-latest-llpr.pt?download=true\",\n        \"models/pet-mad-latest-llpr.pt\",\n    )\n\ncalculator = MetatomicCalculator(\"models/pet-mad-latest-llpr.pt\", device=\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Uncertainties on a Dataset\nThis first example shows how to use PET-MAD to estimate uncertainties on a reference\ndataset. We use a reduced version (because of limited compute power in the CI runner)\nof the MAD validation set.\n\nFor this, we first download the correspond MAD validation dataset record from\nMaterials Cloud. Then, we prepare the dataset and pass it through the model. In the\nfinal step, we visualize the predicted uncertainties and compare them to a\nground truth method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"data/mad-val-100.xyz\"):\n    os.makedirs(\"data\", exist_ok=True)\n    urlretrieve(\n        \"https://huggingface.co/jospies/pet-mad-llpr/resolve/main/mad-val-100.xyz\"\n        \"?download=true\",\n        \"data/mad-val-100.xyz\",\n    )\n\n# Read the dataset's structures.\nsystems = read_systems(\"data/mad-val-100.xyz\")\n\n# Read the dataset's targets.\ntarget_config = {\n    \"energy\": {\n        \"quantity\": \"energy\",\n        \"read_from\": \"data/mad-val-100.xyz\",\n        \"reader\": \"ase\",\n        \"key\": \"energy\",\n        \"unit\": \"kcal/mol\",\n        \"type\": \"scalar\",\n        \"per_atom\": False,\n        \"num_subtargets\": 1,\n        \"forces\": False,\n        \"stress\": False,\n        \"virial\": False,\n    },\n}\ntargets, infos = read_targets(target_config)  # type: ignore\n\n# Wrap in a `metatrain` compatible way.\ndataset = Dataset.from_dict({\"system\": systems, **targets})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After preparation, the dataset can be passed through the model using the calculator\nto obtain energy predictions and LLPR scores.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Convert the systems to an ASE-native `Atoms` object\nsystems = [system_to_ase(sample[\"system\"]) for sample in dataset]\noutputs = {\n    # Request the uncertainty in the atomic energy predictions\n    \"energy\": ModelOutput(),  # (Needed to request the uncertainties)\n    \"energy_uncertainty\": ModelOutput(),\n}\nresults = calculator.run_model(systems, outputs)\n\n# Extract the requested results\npredicted_energies = results[\"energy\"][0].values.squeeze()\npredicted_uncertainties = results[\"energy_uncertainty\"][0].values.squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute the true prediction error by comparing the predicted energy to the reference\nvalue from dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Reference values from dataset.\nground_truth_energies = torch.stack(\n    [sample[\"energy\"][0].values.squeeze() for sample in dataset]\n)\n\n# Compute squared distance between predicted energy and reference value.\nempirical_errors = torch.abs(predicted_energies - ground_truth_energies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After gathering predicted uncertainties and computing ground truth error metrics, we\ncan compare them to each other. Similar to figure S4 of the PET-MAD paper, we present\nthe data in using a parity plot. For more information about interpreting this type of\nplot, see Appendix F.7 of [Bigi et al., 2024](https://arxiv.org/abs/2403.02251).\nNote that both the x- and the y-axis use a logarithmic scale, which is more suitable\nfor inspecting uncertainty values. Because we are using a heavily reduced dataset\n(only 100 structures) from the MAD validation set, the parity plot looks very sparse.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Hard-code the zoomed in region of the plot and iso-lines.\nquantile_lines = [0.00916, 0.10256, 0.4309805, 1.71796, 2.5348, 3.44388]\nmin_val, max_val = 2.5e-2, 2.5\n\n# Create the parity plot.\nplt.figure(figsize=(4, 4))\nplt.grid()\nplt.gca().set(\n    title=\"Parity of Uncertainties\",\n    ylabel=\"Errors\",\n    xlabel=\"Uncertainties\",\n)\nplt.loglog()\n\n# Plot iso lines.\nplt.plot([min_val, max_val], [min_val, max_val], ls=\"--\", c=\"k\")\nfor factor in quantile_lines:\n    plt.plot([min_val, max_val], [factor * min_val, factor * max_val], \"k:\", lw=0.75)\n\n# Add actual samples.\nplt.scatter(predicted_uncertainties, empirical_errors)\n\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Uncertainties in Vacancy Formation Energies\nOne can use ensemble uncertainty quantification to estimate the error in predicting\n[vacancy formation](https://en.wikipedia.org/wiki/Vacancy_defect)\nenergies, which we show in this example.\n\nIn this part, we use an aluminum crystal as an example system. The structure file can\nbe downloaded from\n[Material Project](https://legacy.materialsproject.org/materials/mp-134/)\nas a `.cif` file. We've included such a file with the recipe.\n\nThe following code loads the structure, computes the energy before creating a defect,\ncreates a defect, runs a structural optimization, and computes the energy after the\noptimization. The energy difference can be used to estimate the vacancy formation\nenergy.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load the crystal from the Materials Project and create a supercell (not strictly\n# necessary).\ncrystal_structure = \"data/Al_mp-134_conventional_standard.cif\"\natoms: Atoms = read_cif(crystal_structure)  # type: ignore\nsupercell = atoms * 2\nsupercell.calc = calculator\nN = len(supercell)  # store the number of atoms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now compute the vacancy formation energy by keeping track of the ensemble energies\nat different stages. Note that calling `.get_potential_energy()` on an `Atoms` object\ntriggers computing the ensemble values.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Get ensemble energy before creating the vacancy\noutputs = [\"energy\", \"energy_uncertainty\", \"energy_ensemble\"]\noutputs = {o: ModelOutput() for o in outputs}\nresults = calculator.run_model(supercell, outputs)\nbulk = results[\"energy_ensemble\"][0].values\n\n# Remove an atom (last atom in this case) to create a vacancy\ni = -1\nsupercell.pop(i)\n\n# Get ensemble energy right after creating the vacancy\nresults = calculator.run_model(supercell, outputs)\nright_after_vacancy = results[\"energy_ensemble\"][0].values\n\n# Run structural optimization optimizing both positions and cell layout.\necf = FrechetCellFilter(supercell)\nbfgs = BFGS(ecf)  # type: ignore\nbfgs.run()\n\n# get ensembele energy after optimization\nresults = calculator.run_model(supercell, outputs)\nvacancy = results[\"energy_ensemble\"][0].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute vacancy formation energy for each ensemble member.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vacancy_formation = vacancy - (N - 1) / N * bulk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Put all ensemble energies in a dataframe and compute the desired statistics.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This dataframe contains each stage's energies in a single column.\nnamed_stages = [\n    (\"Before creating vacancy\", bulk),\n    (\"Right after creating vacancy\", right_after_vacancy),\n    (\"Energy of optimized vacancy\", vacancy),\n    (\"Vacancy formation energy\", vacancy_formation),\n]\ndf = pd.DataFrame.from_dict(\n    {\n        # Convert the PyTorch tensors to flat NumPy vectors\n        k: v.detach().numpy().squeeze()\n        for k, v in named_stages\n    }\n)\n\n# Compute statistics (mean, variance, and standard deviation) on the ensemble energies.\ndf = pd.DataFrame(dict(mean=df.mean(), var=df.var(), std=df.std()))\ndf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Uncertainty Propagation with MD\n\nThis example shows how to use i-PI to propagate error estimates from an ensemble to\noutput observables. In this example, we use a box with period boundary conditions\nhousing 32 water molecules. As an observable, we inspect the [Radial Distribution\nFunction (RDF)](https://en.wikipedia.org/wiki/Radial_distribution_function) between\nhydrogen-hydrogen and oxygen-oxygen bonds.\n\nFirst, we run a simulation with i-PI generating a trajectory and logging other\nmetrics. The trajectory and committee energies can be used in a subsequent\npostprocessing step to obtain RDFs using ASE. These can be re-weighted to propagate\nerrors from the committee uncertainties to the observed RDFs.\n\nNote also that we set a `uncertainty_threshold` option in the driver. When running\nfrom the command line, this will output a warning every time one of the atomic energy\nis estimated to have an uncertainty above that threshold (in eV/atom).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load configuration and run simulation.\nwith open(\"data/h2o-32.xml\") as f:\n    xml_input = f.read()\n\n# prints the relevant sections of the input file\nprint(xml_input[:883][-334:])\n\nsim = InteractiveSimulation(xml_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the simulation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# NB: To get better estimates, set this to a higher number (perhaps 10000) to\n# run the simulation for a longer time.\nsim.run(400)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the trajectories and compute the per-frame RDFs\nNote that ASE applies a weird normalization to the partial RDFs,\nwhich require a correction to recover the usual asymptotic\nbehavior at large distances.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "frames: list[Atoms] = ase.io.read(\"h2o-32.pos_0.extxyz\", \":\")  # type: ignore\n\n# Our simulation should only include water molecules. (types: hydrogen=1, oxygen=8)\nassert set(frames[0].numbers.tolist()) == set([1, 8])\n\n# Compute the RDF of each frame (for H-H and for O-O)\nnum_bins = 250\nrdfs_hh = []\nrdfs_oo = []\nxs = None\nfor atoms in frames:\n    # Compute H-H distances\n    bins, xs = ase.geometry.rdf.get_rdf(  # type: ignore\n        atoms, 4.5, num_bins, elements=[1, 1]\n    )\n\n    # smoothen the RDF a bit (not enough data...)\n    bins[2:-2] = (\n        bins[:-4] * 0.1\n        + bins[1:-3] * 0.2\n        + bins[2:-2] * 0.4\n        + bins[3:-1] * 0.2\n        + bins[4:] * 0.1\n    )\n    rdfs_hh.append(bins * 3.0 / 2.0)  # correct ASE normalization\n\n    # Compute O-O distances\n    bins, xs = ase.geometry.rdf.get_rdf(  # type: ignore\n        atoms, 4.5, num_bins, elements=[8, 8]\n    )\n    bins[2:-2] = (\n        bins[:-4] * 0.1\n        + bins[1:-3] * 0.2\n        + bins[2:-2] * 0.4\n        + bins[3:-1] * 0.2\n        + bins[4:] * 0.1\n    )\n    rdfs_oo.append(bins * 3.0)  # correct ASE normalization\n\nrdfs_hh = np.stack(rdfs_hh, axis=0)\nrdfs_oo = np.stack(rdfs_oo, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the i-PI re-weighting utility as a post-processing step.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Save RDFs such that they can be read from i-PI.\nnp.savetxt(\"h2o-32_rdfs_h-h.txt\", rdfs_hh)\nnp.savetxt(\"h2o-32_rdfs_o-o.txt\", rdfs_oo)\n\n# Run the re-weighting tool from i-PI for H-H and O-O\nfor ty in [\"h-h\", \"o-o\"]:\n    infile = f\"h2o-32_rdfs_{ty}.txt\"\n    outfile = f\"h2o-32_rdfs_{ty}_reweighted.txt\"\n    cmd = (\n        f\"i-pi-committee-reweight h2o-32.committee_pot_0 {infile} --input\"\n        \" data/h2o-32.xml\"\n    )\n    print(\"Executing command:\", \"\\t\" + cmd, sep=\"\\n\")\n    cmd = cmd.split()\n    with open(outfile, \"w\") as out:\n        process = subprocess.run(cmd, stdout=out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load and display the RDFs after re-weighting. Note that the results might not noisy\ndue to the small number of MD steps.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load the reweighted RDFs.\nrdfs_hh_reweighted = np.loadtxt(\"h2o-32_rdfs_h-h_reweighted.txt\")\nrdfs_oo_reweighted = np.loadtxt(\"h2o-32_rdfs_o-o_reweighted.txt\")\n\n# Extract columns.\nrdfs_hh_reweighted_mu = rdfs_hh_reweighted[:, 0]\nrdfs_hh_reweighted_err = rdfs_hh_reweighted[:, 1]\nrdfs_hh_reweighted_committees = rdfs_hh_reweighted[:, 2:]\n\nrdfs_oo_reweighted_mu = rdfs_oo_reweighted[:, 0]\nrdfs_oo_reweighted_err = rdfs_oo_reweighted[:, 1]\nrdfs_oo_reweighted_committees = rdfs_oo_reweighted[:, 2:]\n\n# Display results.\nfig, axs = plt.subplots(figsize=(6, 3), sharey=True, ncols=2)\nfor title, ax, mus, std, xlim in [\n    (\"H-H\", axs[0], rdfs_hh_reweighted_mu, rdfs_hh_reweighted_err, (1.0, 4.5)),\n    (\"O-O\", axs[1], rdfs_oo_reweighted_mu, rdfs_oo_reweighted_err, (2.0, 4.5)),\n]:\n    ylabel = \"RDF\" if title == \"H-H\" else None\n    ax.set(title=title, xlabel=\"Distance\", ylabel=ylabel, xlim=xlim, ylim=(-0.1, 3.7))\n    ax.grid()\n    ax.plot(xs, mus, label=\"Mean\", lw=0.5)\n    z95 = 1.96\n    rdfs_ci95 = (mus - z95 * std, mus + z95 * std)\n    ax.fill_between(xs, *rdfs_ci95, alpha=0.5, label=\"CI95\")\n    ax.legend()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}