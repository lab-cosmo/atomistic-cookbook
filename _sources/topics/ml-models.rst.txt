Machine learning models
=======================

This section contains recipes that concern the training of machine-learning
models, or the pre-processing of data to optimize the model architecture or
data.



.. grid:: 1 2 2 3
    :gutter: 1 1 2 3

    .. grid-item::

        .. card:: A ML model for the electron density of states
            :link: ../examples/dos-align/dos-align
            :link-type: doc
            :text-align: center
            :shadow: md

            .. image:: ../examples/dos-align/images/thumb/sphx_glr_dos-align_thumb.png
                :alt: This tutorial would go through the entire machine learning framework for the electronic density of states (DOS). It will cover the construction of the DOS and SOAP descriptors from ase Atoms and eigenenergy results. A simple neural network will then be constructed and the model parameters, along with the energy reference will be optimized during training. A total of three energy reference will be used, the average Hartree potential, the Fermi level, and an optimized energy reference starting from the Fermi level energy reference. The performance of each model is then compared.
                :class: gallery-img

                
    .. grid-item::

        .. card:: Long-distance Equivariants: a tutorial
            :link: ../examples/lode-linear/lode-linear
            :link-type: doc
            :text-align: center
            :shadow: md

            .. image:: ../examples/lode-linear/images/thumb/sphx_glr_lode-linear_thumb.png
                :alt: This tutorial explains how Long range equivariant descriptors can be constructed using featomic and the resulting descriptors be used to construct a linear model with equisolve
                :class: gallery-img

                
    .. grid-item::

        .. card:: Sample and Feature Selection with FPS and CUR
            :link: ../examples/sample-selection/sample-selection
            :link-type: doc
            :text-align: center
            :shadow: md

            .. image:: ../examples/sample-selection/images/thumb/sphx_glr_sample-selection_thumb.png
                :alt: In this tutorial we generate descriptors using featomic, then select a subset of structures using both the farthest-point sampling (FPS) and CUR algorithms implemented in scikit-matter. Finally, we also generate a selection of the most important features using the same techniques.
                :class: gallery-img

                
    .. grid-item::

        .. card:: Periodic Hamiltonian learning
            :link: ../examples/periodic-hamiltonian/periodic-hamiltonian
            :link-type: doc
            :text-align: center
            :shadow: md

            .. image:: ../examples/periodic-hamiltonian/images/thumb/sphx_glr_periodic-hamiltonian_thumb.png
                :alt: This tutorial explains how to train a machine learning model for the electronic Hamiltonian of a periodic system. Even though we focus on periodic systems, the code and techniques presented here can be directly transferred to molecules.
                :class: gallery-img

                
    .. grid-item::

        .. card:: Equivariant linear model for polarizability
            :link: ../examples/polarizability/polarizability
            :link-type: doc
            :text-align: center
            :shadow: md

            .. image:: ../examples/polarizability/images/thumb/sphx_glr_polarizability_thumb.png
                :alt: In this example, we demonstrate how to construct a metatensor atomistic model for the polarizability tensor of molecular systems. This example uses the featomic library to compute equivariant descriptors, and scikit-learn to train a linear regression model. The model can then be used in an ASE calculator. You could also have a look at this recipe based on scalar/tensorial models, which provides an alternative approach for equivariant learning of tensors.
                :class: gallery-img

                
    .. grid-item::

        .. card:: Equivariant model for tensorial properties based on scalar features
            :link: ../examples/learn-tensors-with-mcov/learn-tensors-with-mcov
            :link-type: doc
            :text-align: center
            :shadow: md

            .. image:: ../examples/learn-tensors-with-mcov/images/thumb/sphx_glr_learn-tensors-with-mcov_thumb.png
                :alt: In this example, we demonstrate how to train a metatensor atomistic model on dipole moments and polarizabilities of small molecular systems, using a model that combines scalar descriptors with equivariant tensorial components that depend in a simple way from the molecular geometry. You may also want to read this recipe for a linear polarizability model, which provides an alternative approach for tensorial learning. The model is trained with metatrain and can then be used in an ASE calculator.
                :class: gallery-img

                
    .. grid-item::

        .. card:: The PET-MAD universal potential
            :link: ../examples/pet-mad/pet-mad
            :link-type: doc
            :text-align: center
            :shadow: md

            .. image:: ../examples/pet-mad/images/thumb/sphx_glr_pet-mad_thumb.png
                :alt: This example demonstrates how to use the PET-MAD model with ASE, i-PI and LAMMPS. PET-MAD is a "universal" machine-learning forcefield trained on a dataset that aims to incorporate a very high degree of structural diversity.
                :class: gallery-img

                
    .. grid-item::

        .. card:: MD using direct-force predictions with PET-MAD
            :link: ../examples/pet-mad-nc/pet-mad-nc
            :link-type: doc
            :text-align: center
            :shadow: md

            .. image:: ../examples/pet-mad-nc/images/thumb/sphx_glr_pet-mad-nc_thumb.png
                :alt: Evaluating forces as a direct output of a ML model accelerates their evaluation, by up to a factor of 3 in comparison to the traditional approach that evaluates them as derivatives of the interatomic potential. Unfortunately, as discussed e.g. in this paper, doing so means that forces are not conservative, leading to instabilities and artefacts in many modeling tasks, such as constant-energy molecular dynamics. Here we demonstrate the issues associated with direct force predictions, and ways to mitigate them, using the generally-applicable PET-MAD potential. See also this recipe for examples of using PET-MAD for basic tasks such as geometry optimization and conservative MD, and this for an example of how to use direct forces to accelerate training.
                :class: gallery-img

                
    .. grid-item::

        .. card:: Fine-tuning the PET-MAD universal potential
            :link: ../examples/pet-finetuning/pet-ft
            :link-type: doc
            :text-align: center
            :shadow: md

            .. image:: ../examples/pet-finetuning/images/thumb/sphx_glr_pet-ft_thumb.png
                :alt: This example demonstrates how to fine-tune the PET-MAD universal ML potential on a new dataset using metatrain. This allows adapting the model to a specialized task by retraining it on a more focused, domain-specific dataset.
                :class: gallery-img

                
    .. grid-item::

        .. card:: Conservative fine-tuning for a PET model
            :link: ../examples/pet-finetuning/pet-ft-nc
            :link-type: doc
            :text-align: center
            :shadow: md

            .. image:: ../examples/pet-finetuning/images/thumb/sphx_glr_pet-ft-nc_thumb.png
                :alt: This example demonstrates a "conservative fine-tuning" (or equivalently, "non-conservative pre-training") strategy. This consists in training a model using (faster) direct, non-conservative force predictions, and then fine-tuning it with backpropagated forces to achieve an accurate conservative model.
                :class: gallery-img

                
    .. grid-item::

        .. card:: Long-stride trajectories with a universal FlashMD model
            :link: ../examples/flashmd/flashmd-demo
            :link-type: doc
            :text-align: center
            :shadow: md

            .. image:: ../examples/flashmd/images/thumb/sphx_glr_flashmd-demo_thumb.png
                :alt: For a quickstart on how to use FlashMD with ASE, you can go here.
                :class: gallery-img

                
    .. grid-item::

        .. card:: Computing NMR shielding tensors using ShiftML
            :link: ../examples/shiftml/shiftml-example
            :link-type: doc
            :text-align: center
            :shadow: md

            .. image:: ../examples/shiftml/images/thumb/sphx_glr_shiftml-example_thumb.png
                :alt: This example shows how to compute NMR shielding tensors using a point-edge transformer model trained on the ShiftML dataset.
                :class: gallery-img

                
    .. grid-item::

        .. card:: Hamiltonian Learning for Molecules with Indirect Targets
            :link: ../examples/hamiltonian-qm7/hamiltonian-qm7
            :link-type: doc
            :text-align: center
            :shadow: md

            .. image:: ../examples/hamiltonian-qm7/images/thumb/sphx_glr_hamiltonian-qm7_thumb.png
                :alt: This tutorial introduces a machine learning (ML) framework that predicts Hamiltonians for molecular systems. Another one of our cookbook examples demonstrates an ML model that predicts real-space Hamiltonians for periodic systems. While we use the same model here to predict a molecular Hamiltonians, we further finetune these models to optimise predictions of different quantum mechanical (QM) properties of interest, thereby treating the Hamiltonian predictions as an intermediate component of the ML framework. More details on this hybrid or indirect learning framework can be found in ACS Cent. Sci. 2024, 10, 637âˆ’648. and our preprint arXiv:2504.01187.
                :class: gallery-img

                